# 参数估计

## 最大似然估计

$$
\theta^*=arg\max_{\theta}p(x|\theta)
$$

对$\theta$求导并使用梯度下降

### 离散型随机变量的最大似然估计

![image-20230403205039738](./%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.assets/image-20230403205039738.png)

### 连续型随机变量的最大似然估计

![image-20230403210328536](./%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.assets/image-20230403210328536.png)

## 变分推断

### 平均场理论

将
$$
p(x_1,...,x_n)=p(x_1)p(x_2|x_1)...p(x_n|x_1,x_2)
$$
转换为
$$
q(x_1,...,x_n)=q(x_1)q(x_2)...q(x_n)
$$
使得p，q尽可能相似，从而可以用下面的式子替代上面的式子



平均场理论的核心思想是将后验分布中的每个因子$q_i$看作一个随机变量，然后通过优化每个随机变量的期望来最小化KL散度。具体地，我们假设每个因子分布$q_i$的期望是一个确定的函数$\phi_i$，即$q_i(\theta_i) \approx f(\theta_i;\phi_i)$。然后，我们使用变分推断的方法，通过最小化下面的变分自由能来确定函数$\phi_i$：

$$
F(q) = \int \prod_i q_i(\theta_i) \log\frac{p(\theta,D)}{\prod_i q_i(\theta_i)} d\theta
$$

通过最小化变分自由能，我们可以得到每个因子分布$q_i$的最优解，从而近似后验分布$p(\theta|D)$

### KL散度

### 变分贝叶斯推断

寻找一个恰当的联合分布$Q(x;\theta)$使其替代$p(x)$,衡量近似度需要KL divergence（散度）


$$
KL(Q(x)||P(x|D))=\int Q(x)log\frac{Q(x)}{P(x|D)}dx \\
$$

变形可得

![image-20230409192724505](./%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1.assets/image-20230409192724505.png)

因为KL散度大于等于0，则
$$
-L(Q(x))+logP(D)\ge 0 \\
logP(D) \ge L(Q(x))
$$
最小化KL散度等同于最大化L(Q(x))

$L(Q(x))$可以看成$logP(D)$的下界，这个下界也被称为ELOB

那么最小化$KL(Q(x)||P(x|D)))$可以看成最大化下界的问题



目标函数为
$$
L(Q(x))=\int Q(x)logP(x,D)dx-\int Q(x)logQ(x)dx
$$
- $\int Q(x)logQ(x)dx$变形
  $$
  \int Q(x)logQ(x)dx=\int (\Pi_iQ_i(x_i))\sum_ilogQ_i(x_i)dx\\
  =\sum_j\int (\Pi_iQ_i(x_i))logQ_j(x_j)dx \\
  = ......\\
  =\sum_j\int Q_j(x_j)logQ_j(x_j)dx_j
  $$
  
  可以把改为变量表达式变为低维变量表达式组合的形式
  
- $\int Q(x)logP(x,D)dx$变形

  ???





