# 评估与优化技术

## 欠拟合

模型欠拟合(underfit）或者称这个算法有较高得偏差（high bias）。

优化方法

添加特征

更大的神经网络

减小lambda

## 过拟合

模型过拟合(overfit)或者称这个算法有较高得方差。



![image-20230209210502732](./%E8%AF%84%E4%BC%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF.assets/image-20230209210502732.png)

优化方法：

- 使用更多的训练数据
- 减少训练用得特征
- 减少参数得尺寸
- 添加正则化

![image-20230130003015221](./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.assets/image-20230130003015221.png)

![image-20230130003536513](./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.assets/image-20230130003536513.png)

### L2正则化

减少高阶项得影响

![image-20230130005101159](./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.assets/image-20230130005101159.png)

核心思想是对经验函数(J，cost function)新增关于w的项，因为我们希望最小化J，那么大的系数会使对应得w变小。

![image-20230130010801416](./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.assets/image-20230130010801416.png)

如：

![image-20230130011137135](./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.assets/image-20230130011137135.png)

会使所有得w都会变小。

如果$\lambda$为很小则无效。如果$\lambda$很大则会使所有得w趋近于0，模型则趋近于f(x)=b。

![image-20230209210339538](./%E8%AF%84%E4%BC%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF.assets/image-20230209210339538.png)



![image-20230209201134421](./%E8%AF%84%E4%BC%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF.assets/image-20230209201134421.png)



### 

在神经网络之中，如tanh激活函数，w更小则每个神经元的值更趋近于线性函数，不容易拟合更复杂的函数，可以避免过拟合。

### Dropout Regularization

随机失活正则化，随机使神经元的激活值为0.

```python
di=np.random.arnd(ai.shape[0],ai.shape[1])<keep_prob
ai=ai@di
ai/=keep_prob # 避免下一层的期望值尺度不一样 inverted dropout
```

对于每个神经元，不依赖于上一层任何单个特征，因为每个神经元的值都有可能被丢弃掉，权重得到分散。

### 制造新的数据

数据加工，如图片的裁剪缩放镜像翻转

### 提前终止

绘制开发集和训练集的曲线，在开发集代价上升时停止训练

## 加快训练速度

归一化

### 特征缩放

#### 均值归一化

$$
x_i=\frac{x_i-\mu_i}{max(x_i)-min(x_i)}
$$

表示和均值得差异

#### Z-score 归一化

$$
x_i=\frac{x_i-\mu_i}{\sigma_i}
$$

![image-20230128232650773](./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.assets/image-20230128232650773.png) 



## 梯度消失/爆炸

当神经网络的层数非常多时，初始化W>I则参数会指数级增长，初始化W<I时参数会指数级减少。



因为
$$
a[i][j]=\sum_{j=1}^{n[i-1]}w[i][j]a[i-1][j]
$$
所以期望特征维度越高，值越小。

### 改变初始化方式

当使用relu作为激活函数时，更改初始化w的方式为

```python
W[l]=np.random.randn(shape)*np.sqrt(2/n[l-1])
```

当采用tanh作为激活函数时，改为*np.sqrt(1\*/n[l-1])或

\*np.sqrt(2/(n[l-1]*n[l]))  (Xavier initialization)

## 超参数估计

### 学习率

通常认为学习率在[1e-4,1]之间，

```python
r=-4*np.random.rand()
learning_rate=10**r
```

### 指数平均优化梯度参数

指数平均优化梯度参数，通常认为在[0.9,0.999]之间,可以取随机1-$\beta_1$，在[0.1,0.001]之间

```python
r=-1+-2*np.random.rand() # -1+[0,-2]
beta1=1-10**r #[0.9,0.]
```



## 神经网络

![image-20230209234925402](./%E8%AF%84%E4%BC%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF.assets/image-20230209234925402.png)

 

## 梯度检查

通过模拟计算微分的值和反向传播计算的值作差比较：

<=1e-7则基本是正确的

\>=1e-3则基本是错误的

## 优化方向

以人类水平作为基准

训练误差大于人类水平的部分称为可避免的偏差。

测试误差大于训练误差的部分称为方差variance。

## 错误分析

![image-20230305163441539](./%E8%AF%84%E4%BC%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF.assets/image-20230305163441539.png)

![image-20230306133804049](./%E8%AF%84%E4%BC%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF.assets/image-20230306133804049.png)

