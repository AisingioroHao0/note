# 机器学习方法

机器学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后考虑用什么样的算法求解最优模型。

从给定的，有限的用于学习的训练数据集合出发，假设数据是独立同分布产生的。

并且假设要学习的模型属于某个函数的集合，称为假设空间。

用用某个评价准则，从假设空间中选取一个最优模型，使它对已知的训练数据及未知的测试数据再给定的评价准则下有最优的预测。

## 基本分类

- 监督学习
- 无监督学习
- 强化学习
- 半监督学习
- 主动学习

### 监督学习

指从标注数据中学习预测模型。

标注数据表示输入和输出的对应关系，预测模型对给定的输入产生相应的输出。

监督学习的本质是学习输入到输出的映射的统计规律。

- 输入空间，输出空间，特征空间

  将输入输出看作是定义在输入（特征）空间与输出空间上的随机变量的取值。

  - 输入变量与输出变量均为连续的预测问题称为回归问题。
  - 输入变量为有限个离散的变量的预测问题称为分类问题。
  - 输入变量与输出变量均为离散的预测问题称为标注问题。

- 联合概率分布

  监督学习假设输入与输出的随机变量X和Y遵循联合概率分布P(X,Y)。P(X,Y)表示分布函数或分布密度函数。

- 假设空间

  模型属于由输入空间到输出空间的映射的集合，这样的集合就是假设空间。

- 问题的形式化

  利用训练数据集学习一个模型，再用模型对测试样本集进行预测。

### 无监督学习

指从未标注数据中学习预测模型的机器学习问题。从假设空间中选出在给定评价标准下的最优模型。

### 按模型分类

- 概率模型和非概率模型
- 线性模型和非线性模型
- 参数化模型和非参数化模型

### 按算法分类

- 在线学习
- 批量学习

### 按技巧分类

#### 贝叶斯学习

$$
P(\theta|D)=\frac{P(\theta)P(D|\theta)}{P(D)}
$$

模型估计时，估计整个后验概率分布$P(\theta|D)$。

预测时，计算数据对后验概率分布的期望值。

#### 核方法

使用核函数表示和学习非线性模型的一种机器学习方法

## 机器学习方法三要素

方法=模型+策略+算法

### 模型

所要学习的条件概率分布或决策函数。

模型的假设空间包含所有可能的条件概率分布或决策函数。

假设空间用f表示可以定义为决策函数的集合
$$
F=\{f|Y=f(X)\}
$$

### 策略

#### 损失函数

度量模型一次预测的好坏

- 0-1损失函数
  $$
  L(Y,f(X))=
  \begin{cases}
  1,Y \ne f(X) \\
  0,Y=f(X)
  \end{cases}
  $$

- 平方损失函数
  $$
  L(Y,f(X))=(Y-f(X))^2
  $$

- 绝对损失函数
  $$
  L(Y,f(X))=|Y-f(X)|
  $$

- 对数似然损失函数
  $$
  L(Y,P(Y|X))=-logP(Y|X)
  $$

#### 风险函数

风险函数/期望损失
$$
R_{exp}(f)=E_P[L(Y,f(X))]
=\int_{X \times Y}L(y,f(x))P(x,y)dxdy
$$
经验风险
$$
R_{emp}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))
$$
根据大数定律，当样本容量N趋于无穷时，经验风险$R_{emp}(f)$趋于期望风险$R_{exp}(f)$

当样本不足时，需要对经验风险进行一定的矫正。

##### 经验风险最小化

经验风险最小化(empirical risk minimization,ERM)策略认为，经验风险最小的模型是最优模型。

当样本容量很小时，经验风险最小化会产生“过拟合”现象。

##### 结构风险最小化

防止过拟合现象
$$
R_{srm}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f)
$$
模型f越复杂，复杂度J(f)就越大。

### 算法

学习模型的具体计算方法，

## 监督学习的重要概念

### 模型评估

当损失函数给定时，基于损失函数的模型的训练误差和模型的测试无法就称为学习方法的评估标准。

机器学习方法具体采用的损失函数未必是评估时使用的损失函数。

假设学习到的模型是$Y=\hat{f}(X)$



训练误差：模型$Y=\hat{f}(X)$关于训练数据集的平均损失：
$$
R_{emp}(\hat f)=\frac 1{N}\sum_{i=1}^{N}L(y_i,\hat f(x_i))
$$
测试误差：模型$Y=\hat{f}(X)$关于测试数据集的平均损失：
$$
e_{test}(\hat f)=\frac 1{N'}\sum_{i=1}^{N'}L(y_i,\hat f(x_i))
$$


测试误差反映学习方法对未知的测试数据集的预测能力，也称为泛化能力。

### 模型选择

如果假设空间存在“真”模型，那么所选择的模型应该逼近真模型。

##### 正则化（Regularization）

模型选择的典型方法。

正则化是结构风险最小化策略的实现，是再经验风险上加一个正则化项（regularizer）或罚项（penalty term）。

正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值越大。比如正则化项可以是模型参数向量的范数。

正则化一般具有如下形式
$$
min_{f \in F}\frac {1}{N}\sum_{i=1}^N L(y_i,f(x_i))+\lambda J(f)
$$

#### 交叉验证

如果给定的样本数据重组，进行模型选择的一种简单方法是随机将数据集切分为三个部分：训练集，验证集，测试集。

训练集用于训练模型

验证集用于模型选择

测试集用来对学习方法评估

在学习到不同复杂度的模型中，选择对验证集有最小与预测误差的模型。

##### 简单交叉验证

随机分为两个部分：训练集，测试集。

训练集再各种条件下训练模型获得不同的模型。

在测试集上评价各个模型的误差并选出测试误差最小的模型，

##### S折交叉验证

将已给数据集分为S各互不相交，大小相同的子集。

然后利用S-1个子集的数据训练模型，利用余下的子集测试模型。

将这一过程对可能的S种选择重复进行，最后选出S次评测中平均测试误差最小的模型。

##### 留一交叉验证

令S=N

### 监督学习方法

#### 生成模型

生成方法原理上由数据学习联合概率分布P（X，Y），然后求出条件概率分布P（Y，X）作为预测模型，即生成模型
$$
P(Y|X)=\frac{P(X,Y)}{P(X)}
$$
这样的方法之所以称为生成方法，因为模型给定了输入X产生输出Y的生成关系。

典型的生成模型有朴素贝叶斯法和隐马尔科夫模型。

#### 判别模型

判别方法由数据直接学习决策函数f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。判别方法关心的是戈丁输入X，应该预测什么样的Y。

典型的判别模型包括：k近邻，感知机，逻辑斯谛回归模型，最大熵模型，支持向量机，提升方法和条件随机场等。

#### 区别

生成方法可以还原出联合概率分布P（X，Y），而判别方法不能。

生成方法的的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快地收敛于真是模型。存在隐变量时，仍可以用生成方法学习，判别方法并不好用。

判别方法直接学习条件概率分布P(Y|X)或决策函数f（X），直接面向预测，学习的精确率往往更高。由于直接学习 P(Y|X)或f（X）可以对数据进行各种程度的抽象，定义特征并使用特征，简化学习问题。

