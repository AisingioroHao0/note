# 梯度下降法(gradient descent)

## 基本原理

![image-20230119115326796](./%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.assets/image-20230119115326796.png)

对于含参数w的函数J(w)，求J（w）的最小值：

$\frac{dJ(w)}{dw}$即J在w处的斜率。

当斜率为正w增大J增大，w需要减小。

当斜率为小w增大J减小，w需要增大。

因此前面需要取负。



![image-20230119120744337](./%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.assets/image-20230119120744337.png)

学习率过大容易在不同单调区间反复横跳导致无法逼近到极小值。



每步使用所有训练数据的梯度下降称为批量梯度下降("Batch" gradient decent)。

## 优化方法

特征缩放，如归一化，可以加快求解速度。

个人关于特征缩放可加快求解速度得解释是：

因为损失函数是和训练数据相关的，训练数据的尺度决定了损失函数随训练参数的变化尺度。

如果每个特征的尺度不一样，但每个模型参数变化量级又是固定的，导致每个参数对损失函数的影响程度会不同，如果某个参数对损失函数的影响额外的大，可能会导致损失函数在关于该参数得不同单调区间反复横跳，导致训练速度慢。

![image-20230128000005666](./%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.assets/image-20230128000005666.png)

### 随机梯度下降法

每次下降使用某个数据进行梯度下降，优化某个数据的损失函数，而非所有数据。

优点：加快训练速度

缺点：

- 准确度下降
- 可能收敛到局部最优，单个样本不足以表现所有样本的下降趋势
- 不易于并行实现

### mini-batch梯度下降法

训练集分割为多个小的训练集，每次使用一个训练集进行训练

### Gradient Descent with Momentum

动量梯度下降法，利用指数加权平均法使梯度更加平滑，减缓振荡
$$
V_{dx}=\beta V_{dx}+(1-\beta)dx \\
x-=\alpha V_{dx}
$$



### RMSprop

在动量梯度算法上改进，使大的梯度变小，小的梯度变大
$$
S_{dx}=\beta S_{dx}+(1-\beta)(dx)^2 \\
x-=\alpha \frac{dx}{\sqrt {S_{dx}}}
$$

### adam algorithm

adam：adaptive moment estimation

结合RMSprop与动量梯度
$$
V_{dx}=\beta_1 V_{dx}+(1-\beta_1)dx \\
S_{dx}=\beta_2 S_{dx}+(1-\beta_2)(dx)^2 \\
W=W-\alpha\frac{V_{dw}}{\sqrt{S_{dw}}+\epsilon}
$$


 并使学习率逐渐下降

- epoch_num为梯度下降的次数
- decay_rate

有多种衰减方式：
$$
\alpha=\frac{1}{1+decay\_rate*epoch\_num}\alpha_0 \hfill\\
\alpha=decay\_rate^{epoch\_num}\alpha_0\hfill\\
\alpha=\frac{k}{\sqrt{epoch\_num}}\alpha_0\hfill\\
$$

### batch normalizing

将每层的z值归一化并赋予新的参数使之得到学习能力,学习$\gamma$,$\beta$。

减少了隐藏神经单元的不稳定性，减少了输入值变化产生的不稳定性，增加了神经网络后层的稳定性。

若采用batch normalizing，原先神经网络参数b将无用。

在使用mini-batch时，对每一个mini-batch计算方差于均值进行归一化，方差于均值将产生一定噪声，因此具有轻微正则化的效果。



当一个一个样例测试时，可以使用指数加权平均法估计平均值





## 使用tensorflow实现梯度下降

![image-20230219152918171](./%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.assets/image-20230219152918171.png)

```python
import tensorflow as tf
w = tf.Variable(3.0)
x = 1.0
y = 1.0
alpha = 0.01
iterations = 30
for i in range(iterations):
    with tf.GradientTape() as tape:
        fwb = w*x
        costJ = (fwb - y)**2

    [dJdw] = tape.gradient(costJ, [w])
    w.assigin_add(-alpha*dJdw)

```

## 传统梯度下降法训练缺点

- 梯度越来越稀疏
- 收敛到局部最小值
- 只能监督学习

## 改进

- 逐层构建单层神经元
- 最后调优
