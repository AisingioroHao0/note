# 梯度下降法(gradient descent)

## 基本原理

![image-20230119115326796](./%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.assets/image-20230119115326796.png)

对于含参数w的函数J(w)，求J（w）的最小值：

$\frac{dJ(w)}{dw}$即J在w处的斜率。

当斜率为正w增大J增大，w需要减小。

当斜率为小w增大J减小，w需要增大。

因此前面需要取负。



![image-20230119120744337](./%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.assets/image-20230119120744337.png)

学习率过大容易在不同单调区间反复横跳导致无法逼近到极小值。



每步使用所有训练数据的梯度下降称为批量梯度下降("Batch" gradient decent)。

## 加速下降

特征缩放，如归一化，可以加快求解速度。

个人关于特征缩放可加快求解速度得解释是：

因为损失函数是和训练数据相关的，训练数据的尺度决定了损失函数随训练参数的变化尺度。

如果每个特征的尺度不一样，但每个模型参数变化量级又是固定的，导致每个参数对损失函数的影响程度会不同，如果某个参数对损失函数的影响额外的大，可能会导致损失函数在关于该参数得不同单调区间反复横跳，导致训练速度慢。

![image-20230128000005666](./%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.assets/image-20230128000005666.png)

## adam algorithm

adam：adaptive moment estimation

每个学习参数有一个对应得学习率$\alpha$

自动增大学习率的算法

## 使用tensorflow实现梯度下降

![image-20230219152918171](./%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.assets/image-20230219152918171.png)

```python
import tensorflow as tf
w = tf.Variable(3.0)
x = 1.0
y = 1.0
alpha = 0.01
iterations = 30
for i in range(iterations):
    with tf.GradientTape() as tape:
        fwb = w*x
        costJ = (fwb - y)**2

    [dJdw] = tape.gradient(costJ, [w])
    w.assigin_add(-alpha*dJdw)

```

