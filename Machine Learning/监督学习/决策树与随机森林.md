# 决策树

树型结构，从根节点输入，非叶子节点为决策节点，叶子节点为决策结果。

## 熵的定义

使用熵来衡量节点的纯粹程度

熵函数H(p),p为某个类型对象个数占所有对象的占比

![image-20230214110536533](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230214110536533.png)

当$p_1$为0时H为0,当$p_1$为1时，$H(p_1)$同样为0，因此可写出第一项$p_1log_2(p_1)$，$p_1$的取值为0到1,当log后符号为负因此需要再加一个负号。

同样杂质度由另一项组成，原理同上。

因此熵函数为：

$$
H(p_1)=-p_1log_2(p_1)-p_0log_2(p_0)
$$

## 节点划分

设总个数为cnt，划分到左右的为cntl,cntr，由熵的定义，显然选择节点的划分方式可以通过选择熵减少最大的分裂方式划分
$$
Information Gain=H(p)-(\frac{cntl}{cnt}H(pl)+\frac{cntr}{cnt}H(pr)）
$$

![image-20230215174349502](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230215174349502.png)

对于连续特征，可以枚举训练样本的间隙作为可能的划分。

对于可能取多个值的特征，采用one-hot编码

### 信息增益

对于y取多值有

![image-20230312233101665](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230312233101665.png)

### 信息增益比

以信息增益作为划分训练集数据集的特征，存在偏向于钻则取值较多的特征的问题，使用信息增益比可以对这一问题作修正

![image-20230312233128014](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230312233128014.png)

## 回归树

回归树的叶子节点将根据该节计结束点元素集合作出预测，给出的结果将不是一个离散的值，而是一个数。

比如预测动物的体重，叶子节点将是该节点集合样本所有动物体重的平均值。可令H（p）为方差

## 随机森林

为了避免少量数据的变化便造成树结构的改变，训练多个树作判断。

结果由多个树投票决定。（Tree ensembles）

随机选择样本建立决策树。

并且若有n个特征，令$k=\sqrt{n}$，每次选择k个特征训练一棵决策树



一种以上算法可以避免少量数据敏感的理解是，已经有了不同的数据集训练的决策树，少量数据的敏感性被平均并且已经被肯呢个探索了

## XGBoost（eXtreme Gradient Boosting）

在建立随机森林时，错误的分类样本将有更高的可能被选取训练新的决策树。（boosted tree）

XGBoost是一个开源的实现boosted tree的方法

```python
from xgboost import XGBClassifier
model=XGBClassifier()
model.fit(X_train,y_train)
y_pred=model.predict(X_test)
```

## 决策树与神经网络比较

决策树的优点：

- 决策树适合结构化数据（表格），不适合于非结构化数据（图像，音频，文本等）（关于非结构化的数据，单独拆分某个特征可能无法作出有效划分）
- 训练更快
- 可以直观的被人类理解

神经网络的有点：

- 在非结构化数据上同样表现优秀
- 可以迁移学习
- 当建立多模型系统时，更简单得和其他神经网络串联起来

## 代码

数据集https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction

### 引入相关工具包

```python
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
```

### 使用pandas读取数据

```python
# Load the dataset using pandas
df = pd.read_csv("heart.csv")
df.head()
```

![image-20230217004233261](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230217004233261.png)

### one-hot编码

```python
cat_variables = ['Sex',
'ChestPainType',
'RestingECG',
'ExerciseAngina',
'ST_Slope'
]

# data是使用onehot编码的数据源，columns是使用onhot编码的列，前缀是新生成列名的前缀
df = pd.get_dummies(data = df,
                         prefix = cat_variables,
                         columns = cat_variables)
df.head()
```

![image-20230217005330326](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230217005330326.png)

### 设定预测目标

以预测heartDisease作为目标，因此删除该特征

```python
features = [x for x in df.columns if x not in 'HeartDisease'] ## Removing our target variable
```

### 划分训练集与测试集

使用scikit的 sklearn.model_selection.train_test_split拆分数据，训练集与测试集

```python
X_train, X_val, y_train, y_val = train_test_split(df[features], df['HeartDisease'], train_size = 0.8, random_state = RANDOM_STATE)

# We will keep the shuffle = True since our dataset has not any time dependency.
```

### 使用决策树预测

#### 调试参数

准备调试参数并构建模型：

```python
min_samples_split_list = [2,10, 30, 50, 100, 200, 300, 700] ## If the number is an integer, then it is the actual quantity of samples,
max_depth_list = [1,2, 3, 4, 8, 16, 32, 64, None] # None means that there is no depth limit.
```



- sklearn.tree.DecisionTreeClassifier创建模型
  - min_samples_split
  - max_depth
- model.predict作预测
- sklearn.metrics.accuracy_score计算精确度

##### 不同最小划分数的精确度比较

```python
accuracy_list_train = []
accuracy_list_val = []
for min_samples_split in min_samples_split_list:
    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.
    model = DecisionTreeClassifier(min_samples_split = min_samples_split,
                                   random_state = RANDOM_STATE).fit(X_train,y_train) 
    predictions_train = model.predict(X_train) ## The predicted values for the train dataset
    predictions_val = model.predict(X_val) ## The predicted values for the test dataset
    accuracy_train = accuracy_score(predictions_train,y_train)
    accuracy_val = accuracy_score(predictions_val,y_val)
    accuracy_list_train.append(accuracy_train)
    accuracy_list_val.append(accuracy_val)

plt.title('Train x Test metrics')
plt.xlabel('min_samples_split')
plt.ylabel('accuracy')
plt.xticks(ticks = range(len(min_samples_split_list )),labels=min_samples_split_list)
plt.plot(accuracy_list_train)
plt.plot(accuracy_list_val)
plt.legend(['Train','Test'])
```

![image-20230217010934862](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230217010934862.png)

可以看到规定节点样本个数过少会拟合，过多则会欠拟合。

##### 不同最大树高的精确度比较

```python
accuracy_list_train = []
accuracy_list_val = []
for max_depth in max_depth_list:
    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.
    model = DecisionTreeClassifier(max_depth = max_depth,
                                   random_state = RANDOM_STATE).fit(X_train,y_train) 
    predictions_train = model.predict(X_train) ## The predicted values for the train dataset
    predictions_val = model.predict(X_val) ## The predicted values for the test dataset
    accuracy_train = accuracy_score(predictions_train,y_train)
    accuracy_val = accuracy_score(predictions_val,y_val)
    accuracy_list_train.append(accuracy_train)
    accuracy_list_val.append(accuracy_val)

plt.title('Train x Test metrics')
plt.xlabel('max_depth')
plt.ylabel('accuracy')
plt.xticks(ticks = range(len(max_depth_list )),labels=max_depth_list)
plt.plot(accuracy_list_train)
plt.plot(accuracy_list_val)
plt.legend(['Train','Test'])
```

![image-20230217011457779](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230217011457779.png)

过高则会过拟合，过低则会欠拟合。

#### 模型选择

```python
decision_tree_model = DecisionTreeClassifier(min_samples_split = 50,
                                             max_depth = 3,
                                             random_state = RANDOM_STATE).fit(X_train,y_train)
```

![image-20230217013734402](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230217013734402.png)

### 训练随机森林

RandomForestClassifier创建随机森林模型：

- min_samples_split

- max_depth

- max_features

  随机选择特征的个数

- n_estimators

  决策树的个数

- n_jobs

#### 调试参数

```python
min_samples_split_list = [2,10, 30, 50, 100, 200, 300, 700]  ## If the number is an integer, then it is the actual quantity of samples,
                                             ## If it is a float, then it is the percentage of the dataset
max_depth_list = [2, 4, 8, 16, 32, 64, None]
n_estimators_list = [10,50,100,500]
```

##### 不同最小划分数的精确度比较

```python
accuracy_list_train = []
accuracy_list_val = []
for min_samples_split in min_samples_split_list:
    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.
    model = RandomForestClassifier(min_samples_split = min_samples_split,
                                   random_state = RANDOM_STATE).fit(X_train,y_train) 
    predictions_train = model.predict(X_train) ## The predicted values for the train dataset
    predictions_val = model.predict(X_val) ## The predicted values for the test dataset
    accuracy_train = accuracy_score(predictions_train,y_train)
    accuracy_val = accuracy_score(predictions_val,y_val)
    accuracy_list_train.append(accuracy_train)
    accuracy_list_val.append(accuracy_val)

plt.title('Train x Test metrics')
plt.xlabel('min_samples_split')
plt.ylabel('accuracy')
plt.xticks(ticks = range(len(min_samples_split_list )),labels=min_samples_split_list) 
plt.plot(accuracy_list_train)
plt.plot(accuracy_list_val)
plt.legend(['Train','Test'])
```

![image-20230217012731421](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230217012731421.png)

##### 不同最大的精确度比较

```python
accuracy_list_train = []
accuracy_list_val = []
for max_depth in max_depth_list:
    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.
    model = RandomForestClassifier(max_depth = max_depth,
                                   random_state = RANDOM_STATE).fit(X_train,y_train) 
    predictions_train = model.predict(X_train) ## The predicted values for the train dataset
    predictions_val = model.predict(X_val) ## The predicted values for the test dataset
    accuracy_train = accuracy_score(predictions_train,y_train)
    accuracy_val = accuracy_score(predictions_val,y_val)
    accuracy_list_train.append(accuracy_train)
    accuracy_list_val.append(accuracy_val)

plt.title('Train x Test metrics')
plt.xlabel('max_depth')
plt.ylabel('accuracy')
plt.xticks(ticks = range(len(max_depth_list )),labels=max_depth_list)
plt.plot(accuracy_list_train)
plt.plot(accuracy_list_val)
plt.legend(['Train','Test'])
```

![image-20230217012959866](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230217012959866.png)

##### 不同决策树个数的精确度比较

```python
accuracy_list_train = []
accuracy_list_val = []
for n_estimators in n_estimators_list:
    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.
    model = RandomForestClassifier(n_estimators = n_estimators,
                                   random_state = RANDOM_STATE).fit(X_train,y_train) 
    predictions_train = model.predict(X_train) ## The predicted values for the train dataset
    predictions_val = model.predict(X_val) ## The predicted values for the test dataset
    accuracy_train = accuracy_score(predictions_train,y_train)
    accuracy_val = accuracy_score(predictions_val,y_val)
    accuracy_list_train.append(accuracy_train)
    accuracy_list_val.append(accuracy_val)

plt.title('Train x Test metrics')
plt.xlabel('n_estimators')
plt.ylabel('accuracy')
plt.xticks(ticks = range(len(n_estimators_list )),labels=n_estimators_list)
plt.plot(accuracy_list_train)
plt.plot(accuracy_list_val)
plt.legend(['Train','Test'])
```

![image-20230217013051259](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230217013051259.png)

#### 模型选择

```python
random_forest_model = RandomForestClassifier(n_estimators = 100,
                                             max_depth = 8, 
                                             min_samples_split = 10).fit(X_train,y_train)
print(f"Metrics train:\n\tAccuracy score: {accuracy_score(random_forest_model.predict(X_train),y_train):.4f}\nMetrics test:\n\tAccuracy score: {accuracy_score(random_forest_model.predict(X_val),y_val):.4f}")
```

![image-20230217013718486](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230217013718486.png)

### 自动化调试参数

可以使用sklearn的GridSearchCV

### XGBoost

参数同决策树，多了一个learning_rate被用与内部的梯度下降之中。

- 每一次迭代，使用评估数据集计算cost。

- 一旦cost停止下降多轮，训练将停止。

  更多的迭代会有更多的被训练出来的决策树，选用的是最优的一个，会导致过拟合，通过在cost不再下降时停止可以减少过拟合

.fit参数：

- eval_set
- early_stopping_rounds

#### 数据集拆分

```python
n = int(len(X_train)*0.8) ## Let's use 80% to train and 20% to eval
X_train_fit, X_train_eval, y_train_fit, y_train_eval = X_train[:n], X_train[n:], y_train[:n], y_train[n:]
```

#### 训练模型

```python
xgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.1,verbosity = 1, random_state = RANDOM_STATE)
xgb_model.fit(X_train_fit,y_train_fit, eval_set = [(X_train_eval,y_train_eval)], early_stopping_rounds = 10)
```

#### 查看最优决策树的信息

```python
xgb_model.best_iteration
```

#### 评估模型

```python
print(f"Metrics train:\n\tAccuracy score: {accuracy_score(xgb_model.predict(X_train),y_train):.4f}\nMetrics test:\n\tAccuracy score: {accuracy_score(xgb_model.predict(X_val),y_val):.4f}")
```

![image-20230217015454324](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230217015454324.png)

### 基础算法实现

```python
import numpy as np
import matplotlib.pyplot as plt
from public_tests import *
from utils import *

def compute_entropy(y):
    """
    Computes the entropy for 
    
    Args:
       y (ndarray): Numpy array indicating whether each example at a node is
           edible (`1`) or poisonous (`0`)
       
    Returns:
        entropy (float): Entropy at that node
        
    """
    # You need to return the following variables correctly
    entropy = 0.
    
    ### START CODE HERE ###
    if len(y)==0:
            return 0
    p1=y.sum()/len(y)
    if p1==0 or p1==1:
        return 0
        
    entropy=-p1*np.log2(p1)-(1-p1)*np.log2(1-p1)
        
            
        
            
    ### END CODE HERE ###        
    
    return entropy

# UNQ_C2
# GRADED FUNCTION: split_dataset

def split_dataset(X, node_indices, feature):
    """
    Splits the data at the given node into
    left and right branches
    
    Args:
        X (ndarray):             Data matrix of shape(n_samples, n_features)
        node_indices (list):     List containing the active indices. I.e, the samples being considered at this step.
        feature (int):           Index of feature to split on
    
    Returns:
        left_indices (list):     Indices with feature value == 1
        right_indices (list):    Indices with feature value == 0
    """
    
    # You need to return the following variables correctly
    left_indices = []
    right_indices = []
    
    ### START CODE HERE ###
    for i in node_indices:
        if X[i][feature]:
            left_indices.append(i)
        else:
            right_indices.append(i)
        
            
        
            
    ### END CODE HERE ###
        
    return left_indices, right_indices


def compute_information_gain(X, y, node_indices, feature):
    
    """
    Compute the information of splitting the node on a given feature
    
    Args:
        X (ndarray):            Data matrix of shape(n_samples, n_features)
        y (array like):         list or ndarray with n_samples containing the target variable
        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.
   
    Returns:
        cost (float):        Cost computed
    
    """    
    # Split dataset
    left_indices, right_indices = split_dataset(X, node_indices, feature)
    
    # Some useful variables
    X_node, y_node = X[node_indices], y[node_indices]
    X_left, y_left = X[left_indices], y[left_indices]
    X_right, y_right = X[right_indices], y[right_indices]
    
    # You need to return the following variables correctly
    information_gain = 0
    
    ### START CODE HERE ###
    wl=len(y_left)/len(y_node)
    wr=len(y_right)/len(y_node)
    information_gain=compute_entropy(y_node)-(wl*compute_entropy(y_left)+wr*compute_entropy(y_right))
    
    
    
    
    
    
    
    
    
    ### END CODE HERE ###  
    
    return information_gain

# UNQ_C4
# GRADED FUNCTION: get_best_split

def get_best_split(X, y, node_indices):   
    """
    Returns the optimal feature and threshold value
    to split the node data 
    
    Args:
        X (ndarray):            Data matrix of shape(n_samples, n_features)
        y (array like):         list or ndarray with n_samples containing the target variable
        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.

    Returns: 
        best_feature (int):     The index of the best feature to split
    """    
    
    # Some useful variables
    num_features = X.shape[1]
    
    # You need to return the following variables correctly
    best_feature = -1
    
    ### START CODE HERE ###
    
    max_info_gain = 0

   # Iterate through all features
    for feature in range(num_features): 
        info_gain = compute_information_gain(X, y, node_indices, feature)

        if info_gain > max_info_gain:
            max_info_gain = info_gain
            best_feature = feature     
        
        
            
            
    ### END CODE HERE ##    
   
    return best_feature

# Not graded
tree = []

def build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth):
    """
    Build a tree using the recursive algorithm that split the dataset into 2 subgroups at each node.
    This function just prints the tree.
    
    Args:
        X (ndarray):            Data matrix of shape(n_samples, n_features)
        y (array like):         list or ndarray with n_samples containing the target variable
        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.
        branch_name (string):   Name of the branch. ['Root', 'Left', 'Right']
        max_depth (int):        Max depth of the resulting tree. 
        current_depth (int):    Current depth. Parameter used during recursive call.
   
    """ 

    # Maximum depth reached - stop splitting
    if current_depth == max_depth:
        formatting = " "*current_depth + "-"*current_depth
        print(formatting, "%s leaf node with indices" % branch_name, node_indices)
        return
   
    # Otherwise, get best split and split the data
    # Get the best feature and threshold at this node
    best_feature = get_best_split(X, y, node_indices) 
    
    formatting = "-"*current_depth
    print("%s Depth %d, %s: Split on feature: %d" % (formatting, current_depth, branch_name, best_feature))
    
    # Split the dataset at the best feature
    left_indices, right_indices = split_dataset(X, node_indices, best_feature)
    tree.append((left_indices, right_indices, best_feature))
    
    # continue splitting the left and the right child. Increment current depth
    build_tree_recursive(X, y, left_indices, "Left", max_depth, current_depth+1)
    build_tree_recursive(X, y, right_indices, "Right", max_depth, current_depth+1)
```



## ID3算法

相当于极大似然法进行概率模型的选择，从根节点开始递归建立子节点

![image-20230312232020530](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230312232020530.png)

## C4.5生成算法

与ID3相似，用信息增益比选择特征

![image-20230312233344173](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230312233344173.png)

## 决策树剪枝

防止过拟合问题。

通过极小化决策树整体的损失函数来实现,定义为

![image-20230313002006159](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230313002006159.png)

其实就是 每个节点的元素个数\*这个节点的熵+$\alpha$\*节点个数

剪枝就是$\alpha$确定时，选择损失函数最小的模型，即确定损失函数最小的子树。



自底向上尝试回缩（删除叶子节点将其并入父节点）叶子节点，若回缩后损失函数更小则回缩。

## CART算法

### 生成

#### 回归树

一颗回归树对应着特征空间的一个划分以及在划分的单元上的输出值。

每个单元的输出是在这个单元内的训练集输出的均值。

输入空间划分确认时，可以采用平方误差来表示回归树预测的误差。

![image-20230313142818870](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230313142818870.png)

这样的方法称为最小二乘法：

![image-20230313142948102](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230313142948102.png)

#### 分类树

用基尼指数选择最优特征，同时决定该特征的最优二值切分点。

![image-20230313143940256](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230313143940256.png)

在特征条件A下，集合D的基尼指数为

![image-20230313144316528](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230313144316528.png)

基尼指数Gini(D)表示了集合D的不确定性，基尼指数Gini（D，A）表示经A=a分割后集合D的不确定性。

基尼指数越大，样本集合的不确定性也就越大，和熵相似。

![image-20230313145307410](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230313145307410.png)

### 剪枝

损失函数为

![image-20230313163144559](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230313163144559.png)

![image-20230313163408797](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230313163408797.png)

![image-20230313163427816](./%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.assets/image-20230313163427816.png)
