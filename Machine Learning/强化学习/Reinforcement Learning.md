# 强化学习（reinforcement learning）

## 马卡洛夫决策过程（MDP    Markov Decision Process）

一般用于描述强化学习任务。E=<X,A,P,R>

- E

  机器处于环境E中

- X

  状态空间为X，其中每个状态x是机器感知到的环境的描述

- A

  机器能采取的动作构成了动作空间A，每个动作为a

- P

  a作用于x上，潜在的状态转移函数P使环境从当前状态按某个概率转换到另一个状态

- R

  在转移状态时，环境根据潜在的**奖赏**函数R反馈给机器一个奖赏



机器要做的就是在环境中不断尝试学得一个策略(policy）π，根据这个策略，在状态x下求得执行的动作a=π(x)。

策略有两种表示方法

- π=X->A

  确定性策略常用这种表示方法

- π=X×A->R

  随机性策略通常用这种表示，π(x,a)为状态x下选择动作a的概率

在强化学习中，学习的目的就是要找到能使长期积累奖赏最大化的策略。

长期积累奖赏有多种计算方式：

- T步累计奖赏
  $$
  E[\frac{1}{T} \sum_{t=1}^{T}{r_t}]
  $$

- $\gamma$折扣累计奖赏
  $$
  E[\sum_{t=0}^{+\infty}{\gamma^t r_{t+1}}]
  $$



强化学任务的最终奖赏是多步动作之后才能观察到，不妨先考虑比较简单的情形：最大化单步奖赏。

## K-摇臂赌博机

### 探索与利用

强化学习任务对应了一个理论模型“K-摇臂赌博机”，赌博机有K个摇臂，每在投入硬币后可选择一个摇臂，每个摇臂以一定概率吐出硬币，但赌徒不知道这个概率，目标是通过一定的策略最大化自己的奖赏，即最多的硬币。

- 仅探索法(exploration only)：

  获知每个行为的期望奖赏

  将所有的尝试机会平均分配给每个行为，最后以每个行为各自的平均奖赏作为奖赏期望的近似估计。

- 仅利用法（exploitation only):

  执行奖赏最大的动作，选择到目前为止平均奖赏最大的行为

仅探索会失去很多选择最优行为的机会。仅利用没有很好的估计行为的奖赏，很可能无法选到最优摇臂。

因为选择行为次数有限，加强一方则会削弱另一方，这就是强化学习所面临的"探索利用窘境"(Exploration-Exploitation dilemma) 。



要使累计奖赏最大则要在探索和利用之间达成较好的折中。

### $\epsilon$贪心

每次选择行为时，以$\epsilon$的概率进行探索，以$1-\epsilon$的概率进行利用。

对于每个行为的奖赏，选择n次行为的平均奖赏Q(k)可由下计算。
$$
Q(k)=\frac{1}{n}\sum_{i=1}^{n}{v_i}
$$
可采用增量发计算平均奖赏，更加高效
$$
Q_n(k)=\frac{1}{n}((n-1)\times Q_{n-1}(k)+v_n)
$$
若行为奖赏的不确定性较大，例如概率分布较宽时，则需更多的探索，此时需要较大的$\epsilon$值，

若行为的不确定性较少，例如概率分布比较集中时，则$\epsilon$取较小。

若尝试次数非常大，一段时间后行为的奖赏可以很好的近似出来，可以让$\epsilon$随尝试次数的增加而减少，如$\epsilon=\frac{1}{\sqrt{t}}$

### Softmax算法

基于当前已知行为平均奖赏来对探索和利用进行这种。

若行为的平均奖赏相当，选取行为的概率也相当。

若某些行为的平均奖赏更高，选取该行为的概率也更高。

Softmax行为概率的分配是基于Boltzmann分布
$$
P(k)=\frac
	{
		e^{
			\frac{Q(k)}{\tau}
		}
	}
	{
		\sum_{i=1}^K{
			e^{
				\frac{Q(k)}{\tau}
			}
		}
	}
$$
Q(i)记录当前行为的平均奖赏

$\tau>0$，称为温度：

- 越小则平均奖赏高的行为被选区的概率越高

- 趋于0时，Softmax趋于仅利用

- 趋于无穷大时，Softmax趋于仅探索
  $$
  P(k)=1/k
  $$

## 有模型学习 

机器内部可以模拟出与环境相似的状况，这样称为**模型已知**，在模型已知的环境中学习称为**有模型学习**

### 策略评估

#### 定义

在模型已知时，对任意策略π能估计出该策略带来的期望累积奖赏。

- 状态值函数

  $V^{\pi}(x)$表示从状态x出发，使用策略π所带来的累计奖赏

  - T步累计奖赏
    $$
    V_T^{\pi}(x)=E_{\pi}[\sum_{t=1}^Tr_t |x_0 = x]
    $$

  - $\gamma$折扣累积奖赏
    $$
    V_{\gamma}^{\pi}(x) = E_{\pi}[\sum_{t=0}^{+\infty} \gamma^t r_{t+1} |x_0 = x]
    $$

- 状态动作值函数

  $Q^{\pi}(x，a)$表示从状态x，执行动作a后使用策略π带来的累计奖赏

#### V函数

##### T步累计奖赏计算

若具有马尔可夫性质，即系统下一时刻的状态仅由当前时刻的状态决定，不依赖于以往任何状态，于是值函数具有很简单的递归形式。

对于T步累计奖赏有

![image-20221126191502586](Reinforcement%20Learning.assets/image-20221126191502586.png)

注解：

- π(x, a) = P(action = a|state = x)  

- 相当于把整体期望拆成多个期望之和：

  当前的行为获得奖赏的期望+新的状态累计期望奖赏

对于T步累计奖赏，只需要递归T次就能精确求出值函数。

##### $\gamma$折扣累计奖赏计算

![image-20221126194832681](Reinforcement%20Learning.assets/image-20221126194832681.png)

需要设置一个停止准则。

最常见的是设置一个阈值$\theta$，若执行一次迭代后值函数的改变小于$\theta$则停止。

#### Q函数

Q函数因为动作确定，所以少了一层π(x,a)，只需要枚举a可能造成的状态

![image-20221126203001955](Reinforcement%20Learning.assets/image-20221126203001955.png)

### 策略改进

通过换入最优行为来改进策略。

对于状态x，枚举a，计算Q(x,a)，选取最大的Q(x,a)的a作为π(x)的值。

#### 最优V函数

对于某个策略的累计奖赏进行评估后，若发现它并非最优策略，则希望进行改进。

理想策略应最大化累计奖赏。
$$
\pi^*=arg_\pi max\sum_{x\in X}V^\pi(x)
$$
最优策略对应的值函数$V^*$称为**最优值函数**。

策略空间无约束时，$\forall x \in X :V^*(x)=V^{\pi^*}(x)$

最优V函数可从枚举a改为选取最大的

![image-20221126215515660](Reinforcement%20Learning.assets/image-20221126215515660.png)

即

![image-20221126220057915](Reinforcement%20Learning.assets/image-20221126220057915.png)

#### 最优Q函数

![image-20221126220237063](Reinforcement%20Learning.assets/image-20221126220237063.png)

### 策略迭代与值迭代

从一个初始的随机策略出发，不断重复策略评估于改进策略的操作。

因为策略是选取最优的行为，所以策略和V的改变是一致的，迭代计算V，再根据Q改进策略。

![image-20221205202755932](Reinforcement%20Learning.assets/image-20221205202755932.png)

使用$\gamma $悬赏时，如下更新即可。

![image-20221205202810101](Reinforcement%20Learning.assets/image-20221205202810101.png)

## 免模型学习

不知道环境的转移概率，奖赏函数。

### 蒙特卡洛强化学习

选择动作，观察转移的状态和得到的奖赏。

蒙特卡洛强化学习即多次采样，然后求取平均累计奖赏来作为期望累计奖赏的近似。

需要使用$\epsilon $贪心扩展新的状态和行为。

#### 同策略蒙特卡罗强化学习

先利用当前策略执行T轮并记录历史选择行为、状态、奖赏，然后再更新Q值。伪代码如下

![image-20221205213308255](Reinforcement%20Learning.assets/image-20221205213308255.png)

#### 异策略蒙特卡洛强化学习

轨迹策略是使用$\epsilon $贪心下的结果，想要单纯根据Q带来得奖赏评估更新Q值，不受$\epsilon $得影响，需要一些数学方法。

待补充

![image-20221205214358489](Reinforcement%20Learning.assets/image-20221205214358489.png)

### 时序差分学习

蒙特卡洛强化学习得到一个采用轨迹后再利用这个轨迹更新策略，没有充分利用强化学习任务得MDP结构，可以使用强化学习边执行行为边更新，结合动态规划与蒙特卡洛方法。


$$
Q_{t+1}(x,a)=\frac{1}{t+1}(t*Q_{t}(x,a)+r_{t+1})\\
=\frac{t}{t+1}Q_t(x,a)+\frac{1}{t+1}r_{t+1}\\
=(1-\frac{1}{t+1})Q_t(x,a)+\frac{1}{t+1}r_{t+1}\\
=Q_t(x,a)+\frac{1}{t+1}(r_{t+1}-Q_t(x,a))
$$
对于$\gamma$折扣累计悬赏计算
$$
Q_{t+1}(x,a)=\sum_{i=0}^{t}\gamma^{i}r_{i+1} \\
=r_1+\gamma\sum_{t=1}^{t}\gamma^{i-1}r_{i+1} \\
=R_{x->x'}^a+\gamma V(x') \\
=R_{x->x'}^a+\gamma Q(x',a')
$$
