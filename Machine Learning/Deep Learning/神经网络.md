# 神经网络

神经网络由很多层（layer）组成，每个layer又由很多神经元（Neuron）组成。

每个layer会有一个输入和输出，输出称之为激活值（activation）

激活值由该层的每个神经元的输出组成。

##  前向传播

使用w\[i]\[j]来表示第j个神经元第i维得值

```python
def dense(a_in,W,b):
    units=W.shape[1]
    a_out=np.zeros(units)
    for j in range(units):
        w=W[:,j]
        z=np.dot(w,a_in)+b[j]
        a_out[j]=g(z)
     
def sequential(x):
    a=np.zeros(x.shape[0])
    for layer in range(layers):
```

### 向量化实现

使输入变为一个行向量

每个神经元的参数都为列向量，直接使用矩阵乘法计算

```python
X=np.arrat([[200,17]])
W=np.array([[1,-3,5],
           [-2,4,-6]])
B=np.array([[-1,1,2]])
```

```python
def dense(A_in,W,B):
    Z=np.matmul(A_in,W)+B
    return g(Z)
```

## 激活函数

### Linear activation function

$$
g(z)=z
$$



### Sigmoid

$$
g(z)=\frac {1}{1+e^{-x}}
$$

### ReLU(Rectified Linear Unit)

$$
g(z)=max(0,z)
$$

可以使线性段拼接在一起以模拟复杂的非线性函数，折线函数。

例：

![image-20230203223754523](./%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.assets/image-20230203223754523.png)

输出函数是每个单元相加。

对于单元0当x大于1后，unit0的值便不会变化。因此unit0用来匹配第一段。

对于第二段，使用unit1的值来匹配，希望unit1不影响unit0对目标的贡献，因此显然需要当x大于1时a的值才开始变化，因此b1=-1（第二段的斜率为1,x=1时，x*1-1=0)

对于第三段，使用unit2的值来匹配，同样希望不影响之前的匹配，需要x大于2之后才对目标有贡献，因此b2=-4(第三段的斜率为2，x=2时,x*2-4=0)

### Softmax
