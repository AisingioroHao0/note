# 感知机

二类分类的线性模型.

输入为实例的特征向量,输出为实例的类别

## 感知机模型

由输入空间到输出空间如下函数：
$$
f(x)=sign(w \cdot x + b)
$$

对于感知机有如下解释：
线性方程$w \cdot x +b=0 $对于特征空间$R^n$中的一个超平面S，w是超平面的法向量，b是超平面的截距。

位于这个超平面将特征空间分为两个部分，位于两部分的（特征向量）分别被分为正负两类

![image-20230118202909034](./%E6%84%9F%E7%9F%A5%E6%9C%BA.assets/image-20230118202909034.png)

## 感知机学习策略

首先数据集要有线性可分性，即超平面存在。

### 损失函数定义

损失函数一个选择是误分类点的总数，但是与w、b无关不易优化。因此采用另一个选择，误分类点到超平面S的总距离。

由点到直线距离公式，任意一点到超平面的距离为：
$$
\frac{1}{||w||}|w \cdot x_0 +b|
$$
$||w||$是w的L2范数，即每一项平方求和再开方。

因为误分类点$y_i$与判别$w \cdot x_i +b$不同号，则误分类点到超平面S的距离为：
$$
-\frac{1}{||w||}y_i(w\cdot x_i +b)
$$
假设超平面S的误分类集合为M，那么所有误分类点到超平面S的总距离为:
$$
-\frac{1}{||w||}\sum_{x_i \in M}y_i(w\cdot x_i +b)
$$
可以不考虑$\frac{1}{||w||}$因为不影响单调性

因此损失函数为:
$$
L(w,b)=-\sum_{x_i \in M}y_i(w \cdot x_i + b)
$$


感知及的学习策略就是在假设空间中选取使损失函数最小的模型参数w、b，即感知机模型。

## 感知机学习算法

感知机学习算法是误分类驱动的，具体采用随机梯度下降法

随机选取一个误分类点$(x_i,y_i)$，对w,b进行更新
$$
\frac{\partial L(w,b)}{\partial w}=-\sum_{x_i \in M}y_ix_i \\
\frac{\partial L(w,b)}{\partial b}=-\sum_{x_i \in M}y_i
$$

$$
w=w_+\eta y_ix_i \\
b=b+\eta y_i
$$

### 算法的收敛性证明

对于线性可分的数据集，感知及学习算法的原始形式收敛，经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。

记：
$$
\hat w =(w^T,b)^T\\
\hat x=(x^T,1))^T
$$


Novikoff定理：

设训练数据集$T=\{(x_1,y_1),...\}$是线性可分的则：

- 存在满足条件$||\hat w_{opt}||=1$的超平面$\hat w_{opt} \cdot \hat x$将数据集完全分开；

  （w是划分平面的法向量，所以模长并不影响划分平面的效益，划分平面存在，那么$||\hat w_{opt}||$的超平面也必然存在）

  且存在$\gamma>0$，对所有的数据有$y_i(\hat w \cdot \hat x_i)\ge \gamma$

  （既然存在划分超平面，那么必然无划分错误即$y_i(\hat w \cdot \hat x_i)\ge 0$，显然可以找到一个下界）

- 令$R=\mathop{max}\limits_{1<=i<=N}||\hat x_i||$,则感知及算法在训练数据集的误分类次数k满足不等式
  $$
  k\leq(\frac{R}{\gamma})^2
  $$



证明：



由于数据集是线性可分的，因此超平面存在。

取超平面为$\hat w_{opt}\cdot \hat x=0$，使$||\hat w||=1$。有$y_i(\hat w \cdot \hat x_i)\ge 0$，所以存在
$$
\gamma=min\{y_i (\hat w_i \cdot \hat x_i)\}
$$


使
$$
y_i(\hat w_{opt} \cdot \hat x_i)\ge \gamma
$$


如果实例被误分类，则更新权重。
$$
w=w_+\eta y_ix_i \\
b=b+\eta y_i\\
$$

第k个误分类实例更新权重可写为
$$
\hat w_k=\hat w_{k-1} + \eta y_i\hat x_i
$$
由
$$
\hat w_k=\hat w_{k-1} + \eta y_i\hat x_i \\
y_i(\hat w_{opt} \cdot \hat x_i)\ge \gamma
$$
得
$$
\hat w_k \cdot \hat w_{opt} \hfill \\
=(\hat w_{k-1} + \eta y_i\hat x_i )\cdot w_{opt} \hfill \\
=\hat w_{k-1} \cdot  \hat w_{opt} +\eta y_i  x_i \cdot \hat w_{opt} \hfill\\
\ge \hat w_{k-1}\cdot \hat w_{opt} +\eta \gamma \hfill
$$
递推得
$$
\hat w_k \cdot \hat w_{opt} \hfill \\
\ge \hat w_{k-1}\cdot \hat w_{opt} +\eta \gamma \hfill \\
\ge \hat w_{k-2} \cdot \hat w_{opt} +2\eta \gamma \\
\ge ... \hfill \\
\ge k\eta \gamma \hfill
$$
又
$$
||\hat w_k||^2=||\hat w_{k-1}||^2+2\eta y_i\hat w_{k-1} +\eta^2||\hat x_i||^2 \hfill \\
\leq ||\hat w_{k-1}||^2 + \eta^2R^2 \hfill \\
\leq ||\hat w_{k-2}||^2 + 2\eta^2R^2 \hfill \\
\leq k\eta^2R^2 \hfill \\
$$
由
$$
\hat w_k \cdot \hat w_{opt} \ge k\eta \gamma \hfill \\
||\hat w_k||^2 \le k\eta^2R^2
$$
得
$$
k\eta\gamma \leq\hat w_k \cdot \hat w_{opt} \leq ||\hat w_k||||\hat w_{opt}|| \leq \sqrt{k}\eta R
$$
即
$$
k\leq (\frac {R} {\gamma})^2
$$

### 感知机学习算法的对偶形式

将w和b表示为实例$x_i$和标记$y_i$的现行组合形式，通过求解其系数而求得w和b。

从学习过程过不难看出，最终的w，b可分别被表示为
$$
w=\sum_{i=1}^N \alpha_i y_i x_i \\
b=\sum_{i=1}^N \alpha_i y_i
$$
当$\eta =1$时，$\alpha_i$是数据i被误分类的次数。
$$
f(x)=sign(x(\sum_{i=1}^N \alpha_i y_i x_i )+\sum_{i=1}^N \alpha_i y_i)\\
=sign(\sum_{i=1}^Nx \alpha_i y_i x_i +\alpha_i y_i)
$$


可设感知机模型$f(x)=sign(\sum_{j=1}^Na_jy_jx_j \cdot x+b)$

如果$y_i(\sum_{j=1}^Na_jy_jx_j \cdot x+b)<=0$则
$$
\alpha_i =\alpha_i +\eta \\
b= b+\eta y_i
$$
直至没有误分类数据



对偶形式中训练实例仅以内积的形式出现。

为了方便，可以预先将训练集中实例间的内积计算出来并以矩阵的形式存储，即Gram矩阵
$$
G=[x_i \cdot x_j]_{N \cross N}
$$
