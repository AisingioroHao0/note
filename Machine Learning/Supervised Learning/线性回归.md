# 线性回归（linear regression)

给定n组（x,y），假设x和y为线性关系，并预测模型。

## 模型

$$
f(x)=w\cdot x +b
$$

## 损失函数

使样本到直线的平均距离越小，模型的预测结果准确度越高。

因此损失函数可定义为：
$$
J(w,b)=\frac{1}{m}\sum_{i=1}^m(f(x^{(i)})-y^{(i)})^2
$$
为了求导表示方便，不妨设为，并不影响原模型单调性
$$
J(w,b)=\frac{1}{2m}\sum_{i=1}^m(f(x^{(i)})-y^{(i)})^2
$$
当损失函数最小则模型最优，对样本的预测能力越强。

### 图示

![image-20230126221336514](./%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.assets/image-20230126221336514.png)

J(w,b)的可能图像：

![image-20230126221535243](./%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.assets/image-20230126221535243.png)

当w，b取不同值时，

![image-20230126221642293](./%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.assets/image-20230126221642293.png)

## 学习算法

可以采用梯度下降法求解


$$
\frac{\partial J(w,b)}{\partial w}=\frac{1}{m}\sum_{i=1}^m (f(x^{(i)})-y^{(i)})x^{(i)} \\
\frac{\partial J(w,b)}{\partial b}=\frac{1}{m}\sum_{i=1}^m (f(x^{(i)})-y^{(i)})
$$
即：
$$
w=w-\alpha \frac{1}{m}\sum_{i=1}^m (f(x^{(i)})-y^{(i)})x^{(i)} \\
b=b-\alpha \frac{1}{m}\sum_{i=1}^m (f(x^{(i)})-y^{(i)})
$$

## 多元线性回归

$$
w_1=w_1 - \alpha \frac {1}{m} \sum_{i=1}^{m}(f(x^{(i)})-y^{(i)})x^{(i)}_1 \\
w_2=w_2 - \alpha \frac {1}{m} \sum_{i=1}^{m}(f(x^{(i)})-y^{(i)})x^{(i)}_2 \\
...... \\
w_n=w_n - \alpha \frac {1}{m} \sum_{i=1}^{m}(f(x^{(i)})-y^{(i)})x^{(i)}_n  \\

b=b-\alpha \frac{1}{m}\sum_{i=1}^m(f(x^{(i)})-y^{(i)}) \\
$$

## 代码实现

### 求导

```python
def compute_gradient(X, y, w, b): 
    """
    Computes the gradient for linear regression 
    Args:
      X (ndarray (m,n)): Data, m examples with n features
      y (ndarray (m,)) : target values
      w (ndarray (n,)) : model parameters  
      b (scalar)       : model parameter
      
    Returns:
      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. 
      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. 
    """
    m,n = X.shape           #(number of examples, number of features)
    dj_dw = np.zeros((n,))
    dj_db = 0.
    for i in range(m):                             
        err = (np.dot(X[i], w) + b) - y[i]   
        for j in range(n):                         
            dj_dw[j] = dj_dw[j] + err * X[i, j]    
        dj_db = dj_db + err                        
    dj_dw = dj_dw / m                                
    dj_db = dj_db / m                                
        
    return dj_db, dj_dw
```

### 梯度下降

```python
def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): 
    """
    Performs batch gradient descent to learn w and b. Updates w and b by taking 
    num_iters gradient steps with learning rate alpha
    
    Args:
      X (ndarray (m,n))   : Data, m examples with n features
      y (ndarray (m,))    : target values
      w_in (ndarray (n,)) : initial model parameters  
      b_in (scalar)       : initial model parameter
      cost_function       : function to compute cost
      gradient_function   : function to compute the gradient
      alpha (float)       : Learning rate
      num_iters (int)     : number of iterations to run gradient descent
      
    Returns:
      w (ndarray (n,)) : Updated values of parameters 
      b (scalar)       : Updated value of parameter 
      """
    
    # An array to store cost J and w's at each iteration primarily for graphing later
    J_history = []
    w = copy.deepcopy(w_in)  #avoid modifying global w within function
    b = b_in
    
    for i in range(num_iters):

        # Calculate the gradient and update the parameters
        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None

        # Update Parameters using w, b, alpha and gradient
        w = w - alpha * dj_dw               ##None
        b = b - alpha * dj_db               ##None
      
        # Save cost J at each iteration
        if i<100000:      # prevent resource exhaustion 
            J_history.append( cost_function(X, y, w, b))

        # Print cost every at intervals 10 times or as many iterations if < 10
        if i% math.ceil(num_iters / 10) == 0:
            print(f"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   ")
        
    return w, b, J_history #return final w,b and J history for graphing
```



## 正规方程解法（normal equation）

当特征得数量大于10000后会非常得慢

# 特征缩放

加快求解速度，并且可以用来做多项式回归

![image-20230128232908302](./%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.assets/image-20230128232908302.png)

![image-20230128232822056](./%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.assets/image-20230128232822056.png)

# 多项式回归

可以通过特征所放进行多项式回归，即直接新增x^n的特征到训练集，那样模型参数会增加，最终模型得表现是多项式函数

```python
# create target data
x = np.arange(0, 20, 1)
y = x**2

# engineer features .
X = np.c_[x, x**2, x**3]   #<-- added engineered feature
model_w,model_b = run_gradient_descent_feng(X, y, iterations=10000, alpha=1e-7)

plt.scatter(x, y, marker='x', c='r', label="Actual Value"); plt.title("x, x**2, x**3 features")
plt.plot(x, X@model_w + model_b, label="Predicted Value"); plt.xlabel("x"); plt.ylabel("y"); plt.legend(); plt.show()
```

![image-20230128231731289](./%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.assets/image-20230128231731289.png)

特征$x^2$的图像呈线性，因此模型会和$x^2$呈线性关系。

