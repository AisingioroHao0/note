# Word Representation

## Featurized representation

建立单词的特征表，表示单词在某个方面的，提升泛化能力，相同性质的词之间距离更有助于类比推理



例如寻找近义词：man->woman，寻找king的对应词queen
$$
arg \max_w sim(e_w,e_{king}-(e_{man}-e_{woman}))\\
sim(u,v)=\frac{y^Tv}{||u||_2||v||_2}
$$



## Word2Vec Skip-grams

给出一个语句，随机选择一个词作为上下文词（context），特征矩阵乘以对应的one-hot向量，得出word embedding值然后传入softmax值对某个空位置进行词预测，学习特征表示，word embedding。
$$
p(t|c)=\frac{e^{\theta_t^Te_c}}{\sum_i^N e^{\theta_j^Te_c}}
$$


## 负采样

转换为二元分类算法进行，对每个词预测是否在某个词的上下文中出现

## GloVe（GLobal vectors for word representation）

$x_{ij}$表示词语i在j的上下文中出现
$$
J=minimize \sum_i\sum_jf(x_{ij})(\theta_i^Te_j+b_i+b_j-\log x_{ij})^2
$$

- 当$x_{ij}$为0时，$f(x_{ij})$为0
- 单词ij的相似度受$\log x_{ij}$影响
- 学习出来的特征可能是人类无法理解的



## 减少单词编码的主观偏见

- 检测偏差方向

  $e_i-e_j$取平均值
