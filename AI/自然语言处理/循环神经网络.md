# 序列模型

## 循环神经网络（RNN，Recurrent Neural Network ）

![image-20230413154533057](./%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.assets/image-20230413154533057.png)

符号简化

![image-20230413154611876](./%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.assets/image-20230413154611876.png)

前向传播与反向传播

![image-20230413163021503](./%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.assets/image-20230413163021503.png)

### 不同类型

![image-20230413173950071](./%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.assets/image-20230413173950071.png)

### 语言建模

训练神经网络输出$p(sentence)$。

对训练集的每个句子转换为$y^{<1>},...,y^{<t>}$的形式，每一项可以是one-hot向量或者语料库索引

$x_1$为0向量，预测结果$\hat y^{<1>}$为第一个词的概率，softmax实现，$p(word_1),...,p(word_n)$

$x_t$为训练数据第t-1个词，预测结果$\hat y^{<t>}$为$p(word|sentence[:t-1])$



训练一个语言模型后，可以使用其采样，输入0向量$x_1$，得到softmax分布的第一个词，可以选择其中一个词，并作为$x_2$，直到采样出停止标记词



### 缺点

- 梯度消失，并且对于$\hat y^{<i>}$，受越越远的的$x^{<j>}$影响越小，不适合捕捉长期依赖
- 只能捕捉前面的依赖

## GRU（Gate Recurent Unit）

### simplicied

$$
c^{<t>}=a^{<t>}\\
\tilde c^{<t>}=tanh(W_c[c^{<t-1>},x^{<t>}])+b_c\\

\Gamma_u=\sigma (W_u[c^{<t-1>},x^{<t>}]+b_u) \\
c^{<t>}=\Gamma_u*\tilde c^{<t>}+(1-\Gamma_u)*c^{<t-1>}
$$

 

![image-20230415200005084](./%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.assets/image-20230415200005084.png)

### full

$$
c^{<t>}=a^{<t>}\\
\Gamma_u=\sigma (W_u[c^{<t-1>},x^{<t>}]+b_u) \\
\Gamma_r=\sigma(W_r[c^{<t-1>},x^{<t>}]+b_r) \\
\tilde c^{<t>}=tanh(\Gamma_r W_c[c^{<t-1>},x^{<t>}])+b_c\\
c^{<t>}=\Gamma_u*\tilde c^{<t>}+(1-\Gamma_u)*c^{<t-1>}
$$



## Long SHort Term Memory(LSTM)

$$
c^{<t>}=a^{<t>}\\
\Gamma_u=\sigma (W_u[a^{<t-1>},x^{<t>}]+b_u) \\
\Gamma_f=\sigma(W_f[a^{<t-1>},x^{<t>}]+b_r) \\
\Gamma_o=\sigma(W_o[a^{<t-1>},x^{<t>}]+b_o)\\
\tilde c^{<t>}=tanh(W_c[c^{<t-1>},x^{<t>}]+b_c)\\
c^{<t>}=\Gamma_u*\tilde c^{<t>}+\Gamma_f*c^{<t-1>} \\
a^{<t>}=\Gamma_o tanh(c^{<t>})
$$

![image-20230415202510655](./%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.assets/image-20230415202510655.png)

## 双向循环神经网络（BRNN，Bidirectional RNN）

![image-20230415231325755](./%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.assets/image-20230415231325755.png)

## Deep RNN

![image-20230415231733000](./%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.assets/image-20230415231733000.png)
