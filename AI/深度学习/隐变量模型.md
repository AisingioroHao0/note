# 隐变量模型（latent variable models）

数据点x将被映射为隐变量z，并服从分布$p(z)$

- 先验分布（prior distrubution）$p(z)$
- 似然函数（likelihood）$p(x|z)$
- 联合分布（joint distribution）$p(x,z)=p(x|z)p(z)$
- 边际分布（ marginal distribution）$p(x)$
- 后验分布（posterior distribution）$p(z|x)$

![image-20230423162015741](./%E9%9A%90%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B.assets/image-20230423162015741.png)

- Generation 

  从p(z)中采样z，然从p(x|z)中采样数据点x

- Inference（推断，推理）

  为了推断隐藏变量，从p(x)中采样x，然后从p(z|x)中采样z

  

## 最大似然估计训练隐变量模型

最大化似然函数





![image-20230403200759866](./%E9%9A%90%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B.assets/image-20230403200759866.png)

???。经计算梯度得

![image-20230403210856490](./%E9%9A%90%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B.assets/image-20230403210856490.png)

具体计算则需要先验分布$p(z|x)$。这是一个Inference（推断）问题

近似推理有两种常用方法

- 马尔科夫链蒙特卡洛方法(Markov Chain Monte Carlo methods)
- 变分推断(Variational Inference)

## 变分推断

使用另一个分布$q_{\phi}(z|x)$来估计一个真实的$p_{\theta}(z|x)$。

$q_{\phi}(z|x)$称为变分后验

通过ELBO来最大似然估计,这是一种常用的变分下界
$$
logp_{\theta}(x)\ge E_{q_{\phi}(z)}[log\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}]
$$
即
$$
L_{\theta,\phi}(x)=E_{q_{\phi}(z)}[log\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}]
$$
经过扩展(???)

可得
$$
L_{\theta,\phi}(x)=logp_\theta (x)-KL\left(q_{\phi}(z|x)||p_{\theta}(z|x))\right)
$$
其中
$$
KL(P||Q)=\int_{-\infty}^{\infty}p(x)\log(\frac{p(x)}{q(x)})dx
$$
可以理解为L是对数似然的下界，他们之间的距离即KL散度，又称变分间隙

## 计算ELBO梯度

$$
L_{\theta,\phi}(x)=E_{q_{\phi}(z)}[log\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}]\le logp_\theta(x)
$$

虽然精确计算是可能的，但我们通常使用蒙特卡洛采样法。

从变分后验$q_{\phi}(z|x)$中生成少量样本，并平均，来估计梯度。
$$
\nabla_{\theta}L_{\theta,\phi}(x)=\frac{1}{K}\sum_{i=1}^K \nabla_{\theta}logp_{\theta}(x,z_k)
$$

- $z_k \sim q_{\phi}(z|x)$

这时随机性是在$z_k$上的，无法对$\theta$求导

### 重参数化

期我们无法将概率分布的参数从分布空间移动到期望空间，因此我们希望重写期望使得分布与参数$\theta$无关。

如果考虑$q_{\phi}(z)$为高斯分布，即
$$
z=\mu+\sigma\epsilon
$$

- $\epsilon \sim N(0,1)$，在训练时当作常数

## 总览

假设我们要训练一个深度生成模型，其中$\theta$是生成器的参数，$p_{\theta}(x,z)$是生成器的联合分布，$z$是潜在变量。我们还假设我们有一个编码器，它将输入$x$映射到潜在变量$z$的分布$q_{\phi}(z|x)$，其中$\phi$是编码器的参数。我们的目标是最大化训练集中的数据样本$x$的对数似然$L_{\theta,\phi}(x)$，即

$L_{\theta,\phi}(x) = log p_{\theta}(x) = log\int p_{\theta}(x,z) dz$

但是，这个对数似然的计算是非常困难的，因为我们需要对所有可能的潜在变量$z$进行求和。为了解决这个问题，我们使用变分推断，通过引入一个近似分布$q_{\phi}(z|x)$来近似真实的后验分布$p_{\theta}(z|x) = \frac{p_{\theta}(x,z)}{p_{\theta}(x)}$。这样，我们可以将对数似然表示为

$L_{\theta,\phi}(x) = log p_{\theta}(x) \geq E_{z \sim q_{\phi}(z|x)}[log p_{\theta}(x,z) - log q_{\phi}(z|x)]$

其中，$\geq$表示上界，$E_{z \sim q_{\phi}(z|x)}$表示在潜在变量$z$上的期望。这个上界通常被称为证据下界（evidence lower bound, ELBO），也被称为变分下界（variational lower bound）。

然后，我们可以使用随机梯度下降（SGD）来最大化这个上界，即最大化ELBO。为了计算梯度，我们使用重参数化技巧，将随机变量$z$表示为一个确定性函数$g_{\phi}(\epsilon,x)$和一个噪声变量$\epsilon$的组合，其中$\epsilon$是从标准正态分布$N(0,I)$中采样的。这样，我们可以将梯度表示为

$\nabla_{\theta}L_{\theta,\phi}(x) \approx \frac{1}{K}\sum_{i=1}^K \nabla_{\theta}logp_{\theta}(x,z_k)$

其中，$z_k = g_{\phi}(\epsilon_k,x)$，$\epsilon_k \sim N(0,I)$是从标准正态分布中采样的噪声变量。这个梯度的计算是通过对随机变量$z$进行采样来近似期望值得到的。注意，这里的$\nabla_{\theta}logp_{\theta}(x,z_k)$是可以直接计算的，因为我们可以从生成器$p_{\theta}(x,z)$中抽取样本，并计算其对数概率的导数。

总之，$\nabla_{\theta}L_{\theta,\phi}(x)=\frac{1}{K}\sum_{i=1}^K \nabla_{\theta}logp_{\theta}(x,z_k)$是使用基于变分推断和重参数化技巧的随机梯度下降方法计算深度生成模型的梯度的公式。

## Variational Autoencoders

可观测变量X，隐变量Z，期望得到$p_{\theta}(x|z)$，可以通过学习一个$q_{\phi}(z|x)$得到
$$
p(x)=\int_zp_{\theta}(x|z)p(z)=\int q_{\phi}(z|x)\frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)}=E_{z~q_{\phi}(z|x)}[\frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)}]
$$
类似于[EM算法及其推广.md](../监督学习/EM算法及其推广.md)的推导，可得
$$
log p(x)=\log(E_{z~q_{\phi}(z|x)}[\frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)}])\ge E_{z\sim q_{\phi}(z|x)}[log\frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)}]
$$


### decoder

使用一个神经网络decoder参数化$q_{\phi}(z|x)$

```python
self.decoder = tf.keras.Sequential(
        [
            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),
            tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),
            tf.keras.layers.Reshape(target_shape=(7, 7, 32)),
            tf.keras.layers.Conv2DTranspose(
                filters=64, kernel_size=3, strides=2, padding='same',
                activation='relu'),
            tf.keras.layers.Conv2DTranspose(
                filters=32, kernel_size=3, strides=2, padding='same',
                activation='relu'),
            # No activation
            tf.keras.layers.Conv2DTranspose(
                filters=1, kernel_size=3, strides=1, padding='same'),
        ]
    )

def decode(self, z, apply_sigmoid=False):
    logits = self.decoder(z)
    if apply_sigmoid:
      probs = tf.sigmoid(logits)
      return probs
    return logits

def sample(self, eps=None):
    if eps is None:
      eps = tf.random.normal(shape=(100, self.latent_dim))
    return self.decode(eps, apply_sigmoid=True)
```

### encoder

我们将使用一个平摊变分推断（amortized variational inference）来训练模型。

因此需要另外一个推理网络（encoder），将会参数化似然函数$p_{\theta}(x|z)$

```python
self.encoder = tf.keras.Sequential(
        [
            tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),
            tf.keras.layers.Conv2D(
                filters=32, kernel_size=3, strides=(2, 2), activation='relu'),
            tf.keras.layers.Conv2D(
                filters=64, kernel_size=3, strides=(2, 2), activation='relu'),
            tf.keras.layers.Flatten(),
            # No activation
            tf.keras.layers.Dense(latent_dim + latent_dim),
        ]
    )

def encode(self, x):
    mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)
    return mean, logvar
```

为了从Encoder中生成样例并传给Decoder可以使用重参数化技巧

```python
def reparameterize(self, mean, logvar):
    eps = tf.random.normal(shape=mean.shape)
    return eps * tf.exp(logvar * .5) + mean
```

### 损失函数

```python
def compute_loss(model, x):    mean, logvar = model.encode(x)    z = model.reparameterize(mean, logvar)    x_logit = model.decode(z)
    marginal_likelihood = tf.reduce_sum(x * tf.log(x_logit) + (1-x) * tf.log(1-x_logit),1)    KL_divergence = 0.5* tf.reduce_sum (tf.square(mean) + tf.square(logvar) -tf.log(1e-8   + tf.square(logvar)) -1,1
    ELBO = tf.reduce_mean(marginal_likelihood) - tf.reduce_mean(KL_divergence)    return - ELBO
```
