# 基础

## 变量

想要被优化的量

```python
var=tf.Variable(init_val,dtype)
```

## 计算梯度

```python
with tf.GradientTape() as tape:
    cost = ...
grads=tape.gradient(cost,trainable_variables)
```

## 优化器

```python
tf.keras.optimizers
optimizer=tf.keras.optimizers.adam(learning_rate)
```

### 应用梯度

```python
optimizer.apply_gradients(zip(grads,trainable_variables))
```

### 使用梯度下降优化参数

```python
def cost_fn:
    ...
optimizer.minimize(cost_fn,trainable_variables)
```

## 示例

```python
w=tf.Variable(0,dtype=tf.float32)
optimizer=tf.keras.optimizers.Adam(0.1)
def training(x,w,optimizer):
    with tf.GradientTape() as tape:
        cost=w**2-10*w+25
    grads=tape.gradient(cost,[w])
    optimizer.apply_gradients(zip(grads,[w]))
```

```python
import numpy as np
import tensorflow as tf
w=tf.Variable(0,dtype=tf.float32)
x=np.array([1.0,-10.0,25.0],dtype=np.float32)
optimizer=tf.keras.optimizers.Adam(0.1)
def training(x,w,optimizer):
    def cost_fn():
        return x[0]*w**2+x[1]*w+x[2]
    for i in range(1000):
        optimizer.minimize(cost_fn,[w])
    return w
w=training(x,w,optimizer)
print(w)
```
