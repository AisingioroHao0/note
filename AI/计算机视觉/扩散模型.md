# Diffusion Model

一次到位变为n次到位,增强稳定性

## Denoising Diffusion Probabilistic Models（DDPM）

 概率扩散模型（DDPM）包含前向扩散和后向扩散两个部分。

 前向扩散即对采样的图片多次添加噪声,使数据变为标准高斯分布的过程。

 反向扩散即从标准高斯分布中任意采样,逐步去除噪声的过程生成符合采样分布的图片。

### 前向扩散

初始图像为$x_0$，添加高斯噪声，生成$x_0,x_1,...,x_t$，其中$x_t$对于$x_{t-1}$为高斯分布
$$
q(x_t|x_{t-1})=N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_tI)
$$

- $\beta_t$越大，则和$x_{t-1}$的差别越大
- DDPM的作者采用线性增长$\beta_1=10^{-4},...,\beta_T=0.02$

### 重参数化

假设一个变量符合高斯分布
$$
z\sim N(z,\mu,\sigma^2I)
$$

可以引入随机变量$\epsilon$，等价于
$$
z=\mu+\sigma\cdot\epsilon,\epsilon \sim N(0,I)
$$
因此具体变换可写为
$$
x_t=\sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t}\epsilon_{t-1}
$$

- $\epsilon \sim N(0,I)$

令$\alpha_t=1-\beta_t,\overline \alpha_t=\prod_{i=0}^t\alpha_i$可归纳为
$$
x_t \hfill \\
=\sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t}\epsilon \hfill\\
=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\epsilon\hfill\\
=\sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_{t-1}}\epsilon)+N(0,1-\alpha_t) \hfill \\
=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{\alpha_t(1-\alpha_{t-1})}\epsilon+N(0,1-\alpha_t) \hfill \\
=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+N(0,\alpha_t(1-\alpha_{t-1}))+N(0,1-\alpha_t) \hfill \\
=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+N(0,1-\alpha_t\alpha_{t-1}) \hfill \\
=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_t\alpha_{t-1}}\epsilon \hfill \\
=... \hfill \\
=\sqrt{\alpha_t...\alpha_1}x_0+\sqrt{1-\alpha_t...\alpha_1}\epsilon \hfill \\
=\sqrt{\overline\alpha_t}x_0+\sqrt{1-\overline\alpha_t}\epsilon \hfill \\
$$

因此

$$
q(x_t|x_0)=N(x_t;\sqrt{\overline \alpha_t}x_0,(1-\overline\alpha_t))\\
x_t=\sqrt{\overline\alpha_t}x_0+\sqrt{1-\overline\alpha_t}\epsilon \hfill \\
$$

t越大，$\beta_t$越大，$\alpha_t$越小，因此均值趋近于0，方差趋近于1。
$$
\lim_{t->\infty}(x_t|x_0)=\lim_{t->\infty}q(x_t)=N(0,I)
$$

### 逆向扩散

期望训练出一个$q(x_0)$来根据原来数据分布生成新的数据点

但是$q(x_{t-1}|x_t)$未知，因此使用一个$p_{\theta}$来估计$q(x_{t-1}|x_t)$，q为正态分布，因此选择p为正态分布，并使其均值和方差为参数。

希望由$x_t$变化为$x_0$
$$
p_{\theta}(x_{t-1}|x_t)=N(x_{t-1},\mu_{\theta}(x_t,t),\Sigma_{\theta}(x_t,t)) \\
p_{\theta}(x_{0:T})=p_{\theta}(x_T)\prod_{t=1}^T p_{\theta}(x_{t-1}|x_t)\\
p_\theta(x_0)=\int_{x_1:x_t}p(x_t)p_\theta(x_{t-1|x_t})...p_\theta(x_0|x_1)dx_1:x_t
$$
### 使用一个神经网络来估计反向扩散

使用神经网络来估计p，神经网络的输出即$p_\theta(x)$对应高斯分布得均值，使用最大似然估计反向扩散每一步的方差与均值，

训练神经网络类似于一个变分自动编码机（VAE）。

![image-20230428004735421](./%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.assets/image-20230428004735421.png)

- $q(x_1:x_t|x_0)=q(x_1|x_0)q(x_2|x_2)...q(x_t|x_{t-1})$

....

最大似然估计即优化一个训练数据的负对数似然函数。

使用最大似然估计来估计$p_{\theta}(x_0)$

经

![image-20230523193006776](./%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.assets/image-20230523193006776.png)

证明logp(x)有下界

$$
logp(x)\ge E_{q}(x_1|x_0)[logp_{\theta}(x_0|x_1)]-\\D_{KL}(q(x_T|x_0)||p(x_T))-\\ \sum_{t=2}^T E_{q(x_t|x_0)}[D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))]\\
=L_0-L_T-\sum_{t=2}^T L_{t-1}
$$

- $E_{q}(x_1|x0)[logp_{\theta}(x_0|x_1)]$

- $D_{KL}(q(x_T|x_0)||p(x_T))$

  不含训练参数

因此，最大似然估计转换为最小化$L_{t-1}$



因此，最大似然估计转换为最小化$\sum_{t=1}^{T-1} E_{q(x_t|x_0)}[D_{KL}(q(x_t|x_{t-1},x_0)||p_{\theta}(x_t|x_{t+1}))]$

### 最小化$\sum_{t=1}^{T-1} E_{q(x_t|x_0)}[D_{KL}(q(x_t|x_{t-1},x_0)||p_{\theta}(x_t|x_{t+1}))]$

$L_{t-1}$代表真实分布$q(x_{t-1}|x_t,x_0)$和学习预测出的$p_{\theta}(x_{t-1}|x_t)$的差异

最小化KL散度可以使用公式计算，但有更简单的方法：$q(x_{t-1}|x_t,x_0)$的均值和方差为定值，可以让$q(x_{t-1}|x_t,x_0)$和$p_\theta(x_{t-1}|x_t)$得均值越接近越好，因此期望与求$q(x_{t-1}|x_t,x_0)$分布的均值，并训练神经网络使$G(x_t)$与之越接近越好

使用贝叶斯公式：
$$
q(x_{t-1}|x_t,x_0)=\frac{q(x_{t-1},x_t,x_0)}{q(x_t,x_0)}=\frac{q(x_t|x_{t-1})q(x_{t-1}|x_0)q(x_0)}{q(x_t|x_0)q(x_0)}=\frac{q(x_t|x_{t-1})q(x_{t-1}|x_0)}{q(x_t|x_0)}
$$

可以进行如下变换：

  ![image-20230428212852524](./%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.assets/image-20230428212852524.png)

 

发现$q(x_{t-1}|x_t,x_0)$的数据分布

均值为
$$
\mu(x_{t-1}|x_t,x_0)=
\frac{
		\sqrt \alpha_t(1-\overline \alpha_{t-1})x_t+\sqrt{\overline\alpha_{t-1}}\beta_t x_0
	}
	{
		1-\overline \alpha_t
	}
$$
方差为
$$
\sigma(x_{t-1}|x_t,x_0)=
\frac
{
	(1-\alpha_t)(1-\overline \alpha_{t-1})
}
{
	1-\overline\alpha_t
}
$$


由
$$
x_t=\sqrt{\overline\alpha_t}x_0+\sqrt{1-\overline\alpha_t}\epsilon \rightarrow x_0=\frac
{
	x_t-\sqrt{1-\overline \alpha_t}\epsilon
}
{
	\sqrt{\overline \alpha_t}
}
$$
代入$q(x_{t-1}|x_t,x_0)$均值化简得
$$
x_{t-1}=
\frac
{
	1
}
{
	\sqrt {\alpha_t}
}
(x_t-\frac
		{
			1-\alpha_t
		}
		{
			\sqrt{1-\overline \alpha_t}
		}
		\epsilon
	)
$$


最小化$\sum_{t=2}^T E_{q(x_t|x_0)}[D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))]$即让两个分布尽可能相似，KL散度越小越好，使两个高斯分布KL散度最小等价于使其均值相似，因此我们可以只预测$\epsilon_t$
$$
x_{t-1}=\frac
{
	1
}
{
	\sqrt {\alpha_t}
}
(x_t-\frac
		{
			1-\alpha_t
		}
		{
			\sqrt{1-\overline \alpha_t}
		}
		\epsilon_{\theta}(x_t,t)
	)+\sigma_tz
$$
![image-20230429013317985](./%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.assets/image-20230429013317985.png)

## 其他场景的应用

- WaveGrad

  语音合成

- Diffusion-LM

  文本生成，添加噪声到隐变量空间

- Diffusioin via Edit-based Reconstruction(EiffusER)

  文本生成，对文本添加mask噪声

## 类似方法

### Mask-Predict

训练一个图像的AutoEncoder

训练时，把一张图片的token某些部分换成masktoken，训练一个BidrectionalTransformer网络，再把mask预测出来

在生成时，使用全部是mask的token为初始，使用BidrectionalTransformer填充mask，并保留置信度高的填充结果，置信度低的再置为mask投入BidrectionalTransformer

## Stable Diffusion

在隐空间下进行ddpm，减小ddpm模型所需要的运算量，并且更加稳定

![image-20230522155757606](./%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.assets/image-20230522155757606.png)







### TextTransformer

使用基于Transofrmer的文本编码器，以实现条件控制，文本生成图像，使用CLIP的结构，可以使用预训练参数


### autoencoder

将图像映射到隐空间下，使生成更加稳定

```python
class PaddedConv2D(keras.layers.Layer):
    def __init__(self, channels, kernel_size, padding=0, stride=1):
        super().__init__()
        self.padding2d = keras.layers.ZeroPadding2D((padding, padding))
        self.conv2d = keras.layers.Conv2D(
            channels, kernel_size, strides=(stride, stride)
        )

    def call(self, x):
        x = self.padding2d(x)
        return self.conv2d(x)

class AttentionBlock(keras.layers.Layer):
    def __init__(self, channels):
        super().__init__()
        self.norm = tfa.layers.GroupNormalization(epsilon=1e-5)
        self.q = keras.layers.Conv2D(channels, 1,strides=(1,1))
        self.k = keras.layers.Conv2D(channels, 1,strides=(1,1))
        self.v = keras.layers.Conv2D(channels, 1,strides=(1,1))
        self.proj_out = keras.layers.Conv2D(channels, 1,strides=(1,1))

    def call(self, x):
        h_ = self.norm(x)
        q, k, v = self.q(h_), self.k(h_), self.v(h_)

        # Compute attention
        b, h, w, c = q.shape
        q = tf.reshape(q, (-1, h * w, c))  # b,hw,c
        k = keras.layers.Permute((3, 1, 2))(k)
        k = tf.reshape(k, (-1, c, h * w))  # b,c,hw
        w_ = q @ k
        w_ = w_ * (c ** (-0.5))
        w_ = keras.activations.softmax(w_)

        # Attend to values
        v = keras.layers.Permute((3, 1, 2))(v)
        v = tf.reshape(v, (-1, c, h * w))
        w_ = keras.layers.Permute((2, 1))(w_)
        h_ = v @ w_
        h_ = keras.layers.Permute((2, 1))(h_)
        h_ = tf.reshape(h_, (-1, h, w, c))
        return x + self.proj_out(h_)

class ResnetBlock(keras.layers.Layer):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.norm1 = tfa.layers.GroupNormalization(epsilon=1e-5)
        self.conv1 = PaddedConv2D(out_channels, 3, padding=1)
        self.norm2 = tfa.layers.GroupNormalization(epsilon=1e-5)
        self.conv2 = PaddedConv2D(out_channels, 3, padding=1)
        self.nin_shortcut = (
            PaddedConv2D(out_channels, 1)
            if in_channels != out_channels
            else lambda x: x
        )

    def call(self, x):
        h = self.conv1(keras.activations.swish(self.norm1(x)))
        h = self.conv2(keras.activations.swish(self.norm2(h)))
        return self.nin_shortcut(x) + h    
    
class Decoder(keras.Sequential):
    def __init__(self):
        super().__init__(
            [
                keras.layers.Lambda(lambda x: 1 / 0.18215 * x),
                PaddedConv2D(4, 1),
                PaddedConv2D(512, 3, padding=1),
                ResnetBlock(512, 512),
                AttentionBlock(512),
                ResnetBlock(512, 512),
                ResnetBlock(512, 512),
                ResnetBlock(512, 512),
                ResnetBlock(512, 512),
                keras.layers.UpSampling2D(size=(2, 2)),
                PaddedConv2D(512, 3, padding=1),
                ResnetBlock(512, 512),
                ResnetBlock(512, 512),
                ResnetBlock(512, 512),
                keras.layers.UpSampling2D(size=(2, 2)),
                PaddedConv2D(512, 3, padding=1),
                ResnetBlock(512, 256),
                ResnetBlock(256, 256),
                ResnetBlock(256, 256),
                keras.layers.UpSampling2D(size=(2, 2)),
                PaddedConv2D(256, 3, padding=1),
                ResnetBlock(256, 128),
                ResnetBlock(128, 128),
                ResnetBlock(128, 128),
                tfa.layers.GroupNormalization(epsilon=1e-5),
                keras.layers.Activation("swish"),
                PaddedConv2D(3, 3, padding=1),
            ]
        )

class Encoder(keras.Sequential):
    def __init__(self):
        super().__init__(
            [
                PaddedConv2D(128, 3, padding=1 ),
                ResnetBlock(128,128),
                ResnetBlock(128, 128),
                PaddedConv2D(128 , 3 , padding=(0,1), stride=2),
                
                ResnetBlock(128,256),
                ResnetBlock(256, 256),
                PaddedConv2D(256 , 3 , padding=(0,1), stride=2),
                
                ResnetBlock(256,512),
                ResnetBlock(512, 512),
                PaddedConv2D(512 , 3 , padding=(0,1), stride=2),
                
                ResnetBlock(512,512),
                ResnetBlock(512, 512),
                
                ResnetBlock(512, 512),
                AttentionBlock(512),
                ResnetBlock(512, 512),
                
                tfa.layers.GroupNormalization(epsilon=1e-5) , 
                keras.layers.Activation("swish"),
                PaddedConv2D(8, 3, padding=1 ),
                PaddedConv2D(8, 1 ),
                keras.layers.Lambda(lambda x : x[... , :4] * 0.18215)
            ]
        )

```



### diffusion_model




## 参考

### 高斯分布KL散度

两个单一变量的高斯分ff布p和q而言，
$$
KL(p,q)=log\frac{\sigma_2}{\sigma_1}+\frac{\sigma^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}-\frac{1}{2}
$$

### ELBO

ELBO（Evidence Lower Bound）是一种在变分推断中常用的优化目标函数，用于近似计算模型的边缘对数似然。

假设我们有一个概率模型 $p(\mathbf{X}, \mathbf{Z})$，其中 $\mathbf{X}$ 是观测数据，$\mathbf{Z}$ 是潜在变量。我们希望计算 $p(\mathbf{X})$ 的对数似然 $\log p(\mathbf{X})$。但是，由于 $\mathbf{Z}$ 是未知的，我们无法直接计算边缘概率 $p(\mathbf{X})$。变分推断是一种用来逼近边缘概率分布的方法，它利用一个近似分布 $q(\mathbf{Z})$ 来近似真实的后验分布 $p(\mathbf{Z}|\mathbf{X})$。

ELBO 是一种用来评价近似分布 $q(\mathbf{Z})$ 和真实后验分布 $p(\mathbf{Z}|\mathbf{X})$ 之间的差异的指标。ELBO 定义为：

$$\text{ELBO}(q) = \mathbb{E}_{q(\mathbf{Z})}[\log p(\mathbf{X}, \mathbf{Z}) - \log q(\mathbf{Z})]$$

其中，$\mathbb{E}_{q(\mathbf{Z})}$ 表示对 $q(\mathbf{Z})$ 取期望。ELBO 可以被理解为真实后验分布的对数似然 $\log p(\mathbf{X})$ 的下界。具体来说，ELBO 满足下面的不等式：

$$\log p(\mathbf{X}) \geq \text{ELBO}(q)$$

也就是说，ELBO 给出了一个下界，它可以用来近似计算 $\log p(\mathbf{X})$。



log p(x)下界求得如下

### 参数重整化

希望从高斯分布$N(\mu,\sigma)$中采样，可以先从标注只能分布$N(0,1)$采样出Z，在得到$\sigma z+\mu$，这样做将随机性转移到z这个常量上，而$\sigma,\mu$当做仿射变换网络的一部分
