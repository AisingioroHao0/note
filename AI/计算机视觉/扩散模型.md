# Diffusion Model

## Denoising Diffusion Probabilistic Models（DDPM）

包含前向扩散和后向扩散两个部分。

前向扩散即多次添加噪声。

后向扩散即使用神经网络反转添加噪声的过程。

生成使用后向扩散，每一步后向扩散都会在

### 前向扩散

初始图像为$x_0$，添加高斯噪声，生成$x_0,x_1,...,x_t$，其中$x_t$对于$x_{t-1}$为高斯分布
$$
q(X_t|x_{t-1})=N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_tI)
$$

- $\beta_t$越大，则和$x_{t-1}$的差别越大
- DDPM的作者采用线性增长$\beta_1=10^{-4},...,\beta_T=0.02$

#### 重参数化

假设一个变量符合高斯分布
$$
z\sim N(z,\mu,\sigma^2I)
$$

可以引入随机变量$\epsilon$，等价于
$$
z=\mu+\sigma\cdot\epsilon,\epsilon \sim N(0,I)
$$
因此具体变换可写为
$$
x_t=\sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t}\epsilon_{t-1}
$$

- $\epsilon \sim N(0,I)$

令$\alpha_t=1-\beta_t,\overline \alpha_t=\prod_{i=0}^t\alpha_ii$可归纳为
$$
x_t \hfill \\
=\sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t}\epsilon \hfill\\
=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\epsilon\hfill\\
=\sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_{t-1}}\epsilon)++N(0,1-\alpha_t) \hfill \\
=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{\alpha_t(1-\alpha_{t-1})}\epsilon++N(0,1-\alpha_t) \hfill \\
=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+N(0,\alpha_t(1-\alpha_{t-1}))+N(0,1-\alpha_t) \hfill \\
=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+N(0,1-\alpha_t\alpha_{t-1}) \hfill \\
=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_t\alpha_{t-1}}\epsilon \hfill \\
=... \hfill \\
=\sqrt{\alpha_t...\alpha_1}x_0+\sqrt{1-\alpha_t...\alpha_1}\epsilon \hfill \\
=\sqrt{\overline\alpha_t}x_0+\sqrt{1-\overline\alpha_t}\epsilon \hfill \\
$$

因此

$$
q(x_t|x_0)=N(x_t;\sqrt{\overline \alpha_t}x_0,(1-\overline\alpha_t))
$$

t越大，$\beta_t$越大，$\alpha_t$越小，因此均值趋近于0，方差趋近于1。
$$
\lim_{t->\infty}(x_t|x_0)=\lim_{t->\infty}q(x_t)=N(0,I)
$$

### 逆向扩散

期望训练出一个$q(x_0)$来根据原来数据分布生成新的数据点

但是$q(x_{t-1}|x_t)$未知，因此使用一个$p_{\theta}$来估计$q(x_{t-1}|x_t)$，q为正态分布，因此选择p为正态分布，并使其均值和方差为参数。

希望由$x_t$变化为$x_0$
$$
p_{\theta}(x_{t-1}|x_t)=N(x_{t-1},\mu_{\theta}(x_t,t),\Sigma_{\theta}(x_t,t)) \\
p_{\theta}(x_{0:T})=p_{\theta}(x_T)\prod_{t=1}^T p_{\theta}(x_{t-1}|x_t)
$$
#### 使用一个神经网络来估计反向扩散

使用神经网络来估计p，使用最大似然估计每一步的方差与均值，

训练神经网络类似于一个变分自动编码机（VAE）。

当有$x_0$后，可以使用贝叶斯公式反推数据分布,把逆向过程用前向过程表示
$$
p(x_{t-1}|x_t,x_0)=\frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)}
$$

....

最大似然估计即优化一个训练数据的负对数似然函数。
$$
L(x)=-logp(x)
$$
可以证明logp(x)有一个下界
$$
logp(x)\ge E_{q}(x_1|x0)[logp_{\theta}(x_0|x_1)]-\\D_{KL}(q(x_T|x_0)||p(x_T))-\\ \sum_{t=2}^T E_{q(x_t|x_0)}[D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))]\\
=L_0-L_T-\sum_{t=2}^T L_{t-1}
$$

- $E_{q}(x_1|x0)[logp_{\theta}(x_0|x_1)]$

  重构项，使用独立解码器学习

- $D_{KL}(q(x_T|x_0)||p(x_T))$

  衡量$x_T$有多符合高斯分布
  
- $L_{t-1}$代表真实分布$q(x_{t-1}|x_t,x_0)$和学习预测出的$p_{\theta}(x_{t-1}|x_t)$的差异



## 参考

### 高斯分布KL散度

两个单一变量的高斯分布p和q而言，
$$
KL(p,q)=log\frac{\sigma_2}{\sigma_1}+\frac{\sigma^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}-\frac{1}{2}
$$

### ELBO

ELBO（Evidence Lower Bound）是一种在变分推断中常用的优化目标函数，用于近似计算模型的边缘对数似然。

假设我们有一个概率模型 $p(\mathbf{X}, \mathbf{Z})$，其中 $\mathbf{X}$ 是观测数据，$\mathbf{Z}$ 是潜在变量。我们希望计算 $p(\mathbf{X})$ 的对数似然 $\log p(\mathbf{X})$。但是，由于 $\mathbf{Z}$ 是未知的，我们无法直接计算边缘概率 $p(\mathbf{X})$。变分推断是一种用来逼近边缘概率分布的方法，它利用一个近似分布 $q(\mathbf{Z})$ 来近似真实的后验分布 $p(\mathbf{Z}|\mathbf{X})$。

ELBO 是一种用来评价近似分布 $q(\mathbf{Z})$ 和真实后验分布 $p(\mathbf{Z}|\mathbf{X})$ 之间的差异的指标。ELBO 定义为：

$$\text{ELBO}(q) = \mathbb{E}_{q(\mathbf{Z})}[\log p(\mathbf{X}, \mathbf{Z}) - \log q(\mathbf{Z})]$$

其中，$\mathbb{E}_{q(\mathbf{Z})}$ 表示对 $q(\mathbf{Z})$ 取期望。ELBO 可以被理解为真实后验分布的对数似然 $\log p(\mathbf{X})$ 的下界。具体来说，ELBO 满足下面的不等式：

$$\log p(\mathbf{X}) \geq \text{ELBO}(q)$$

也就是说，ELBO 给出了一个下界，它可以用来近似计算 $\log p(\mathbf{X})$。

### 参数重整化

希望从高斯分布$N(\mu,\sigma)$中采样，可以先从标注只能分布$N(0,1)$采样出Z，在得到$\sigma z+\mu$，这样做将随机性转移到z这个常量上，而$\sigma,\mu$当做仿射变换网络的一部分
