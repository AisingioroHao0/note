# Diffusion Model

一次到位变为n次到位,增强稳定性

## Denoising Diffusion Probabilistic Models（DDPM）

 概率扩散模型（DDPM）包含前向扩散和后向扩散两个部分。

 前向扩散即对采样的图片多次添加噪声,使数据变为标准高斯分布的过程。

 反向扩散即从标准高斯分布中任意采样,逐步去除噪声的过程生成符合采样分布的图片。

### 前向扩散

初始图像为$x_0$，添加高斯噪声，生成$x_0,x_1,...,x_t$，其中$x_t$对于$x_{t-1}$为高斯分布
$$
q(x_t|x_{t-1})=N(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_tI)
$$

- $\beta_t$越大，则和$x_{t-1}$的差别越大
- DDPM的作者采用线性增长$\beta_1=10^{-4},...,\beta_T=0.02$

### 重参数化

假设一个变量符合高斯分布
$$
z\sim N(z,\mu,\sigma^2I)
$$

可以引入随机变量$\epsilon$，等价于
$$
z=\mu+\sigma\cdot\epsilon,\epsilon \sim N(0,I)
$$
因此具体变换可写为
$$
x_t=\sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t}\epsilon_{t-1}
$$

- $\epsilon \sim N(0,I)$

令$\alpha_t=1-\beta_t,\overline \alpha_t=\prod_{i=0}^t\alpha_i$可归纳为
$$
x_t \hfill \\
=\sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t}\epsilon \hfill\\
=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\epsilon\hfill\\
=\sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_{t-1}}\epsilon)+N(0,1-\alpha_t) \hfill \\
=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{\alpha_t(1-\alpha_{t-1})}\epsilon+N(0,1-\alpha_t) \hfill \\
=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+N(0,\alpha_t(1-\alpha_{t-1}))+N(0,1-\alpha_t) \hfill \\
=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+N(0,1-\alpha_t\alpha_{t-1}) \hfill \\
=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_t\alpha_{t-1}}\epsilon \hfill \\
=... \hfill \\
=\sqrt{\alpha_t...\alpha_1}x_0+\sqrt{1-\alpha_t...\alpha_1}\epsilon \hfill \\
=\sqrt{\overline\alpha_t}x_0+\sqrt{1-\overline\alpha_t}\epsilon \hfill \\
$$

因此

$$
q(x_t|x_0)=N(x_t;\sqrt{\overline \alpha_t}x_0,(1-\overline\alpha_t))\\
x_t=\sqrt{\overline\alpha_t}x_0+\sqrt{1-\overline\alpha_t}\epsilon \hfill \\
$$

t越大，$\beta_t$越大，$\alpha_t$越小，因此均值趋近于0，方差趋近于1。
$$
\lim_{t->\infty}(x_t|x_0)=\lim_{t->\infty}q(x_t)=N(0,I)
$$

### 逆向扩散

期望训练出一个$q(x_0)$来根据原来数据分布生成新的数据点

但是$q(x_{t-1}|x_t)$未知，因此使用一个$p_{\theta}$来估计$q(x_{t-1}|x_t)$，q为正态分布，因此选择p为正态分布，并使其均值和方差为参数。

希望由$x_t$变化为$x_0$
$$
p_{\theta}(x_{t-1}|x_t)=N(x_{t-1},\mu_{\theta}(x_t,t),\Sigma_{\theta}(x_t,t)) \\
p_{\theta}(x_{0:T})=p_{\theta}(x_T)\prod_{t=1}^T p_{\theta}(x_{t-1}|x_t)\\
p_\theta(x_0)=\int_{x_1:x_t}p(x_t)p_\theta(x_{t-1|x_t})...p_\theta(x_0|x_1)dx_1:x_t
$$
### 使用一个神经网络来估计反向扩散

使用神经网络来估计p，神经网络的输出即$p_\theta(x)$对应高斯分布得均值，使用最大似然估计反向扩散每一步的方差与均值，

训练神经网络类似于一个变分自动编码机（VAE）。

![image-20230428004735421](./%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.assets/image-20230428004735421.png)

- $q(x_1:x_t|x_0)=q(x_1|x_0)q(x_2|x_2)...q(x_t|x_{t-1})$

....

最大似然估计即优化一个训练数据的负对数似然函数。

使用最大似然估计来估计$p_{\theta}(x_0)$

经

![image-20230523193006776](./%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.assets/image-20230523193006776.png)

证明logp(x)有下界

$$
logp(x)\ge E_{q}(x_1|x_0)[logp_{\theta}(x_0|x_1)]-\\D_{KL}(q(x_T|x_0)||p(x_T))-\\ \sum_{t=2}^T E_{q(x_t|x_0)}[D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))]\\
=L_0-L_T-\sum_{t=2}^T L_{t-1}
$$

- $E_{q}(x_1|x0)[logp_{\theta}(x_0|x_1)]$

- $D_{KL}(q(x_T|x_0)||p(x_T))$

  不含训练参数

因此，最大似然估计转换为最小化$L_{t-1}$



因此，最大似然估计转换为最小化$\sum_{t=1}^{T-1} E_{q(x_t|x_0)}[D_{KL}(q(x_t|x_{t-1},x_0)||p_{\theta}(x_t|x_{t+1}))]$

### 最小化$\sum_{t=1}^{T-1} E_{q(x_t|x_0)}[D_{KL}(q(x_t|x_{t-1},x_0)||p_{\theta}(x_t|x_{t+1}))]$

$L_{t-1}$代表真实分布$q(x_{t-1}|x_t,x_0)$和学习预测出的$p_{\theta}(x_{t-1}|x_t)$的差异

最小化KL散度可以使用公式计算，但有更简单的方法：$q(x_{t-1}|x_t,x_0)$的均值和方差为定值，可以让$q(x_{t-1}|x_t,x_0)$和$p_\theta(x_{t-1}|x_t)$得均值越接近越好，因此期望与求$q(x_{t-1}|x_t,x_0)$分布的均值，并训练神经网络使$G(x_t)$与之越接近越好

使用贝叶斯公式：
$$
q(x_{t-1}|x_t,x_0)=\frac{q(x_{t-1},x_t,x_0)}{q(x_t,x_0)}=\frac{q(x_t|x_{t-1})q(x_{t-1}|x_0)q(x_0)}{q(x_t|x_0)q(x_0)}=\frac{q(x_t|x_{t-1})q(x_{t-1}|x_0)}{q(x_t|x_0)}
$$

可以进行如下变换：

  ![image-20230428212852524](./%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.assets/image-20230428212852524.png)

 

发现$q(x_{t-1}|x_t,x_0)$的数据分布

均值为
$$
\mu(x_{t-1}|x_t,x_0)=
\frac{
		\sqrt \alpha_t(1-\overline \alpha_{t-1})x_t+\sqrt{\overline\alpha_{t-1}}\beta_t x_0
	}
	{
		1-\overline \alpha_t
	}
$$
方差为
$$
\sigma(x_{t-1}|x_t,x_0)=
\frac
{
	(1-\alpha_t)(1-\overline \alpha_{t-1})
}
{
	1-\overline\alpha_t
}
$$


由
$$
x_t=\sqrt{\overline\alpha_t}x_0+\sqrt{1-\overline\alpha_t}\epsilon \rightarrow x_0=\frac
{
	x_t-\sqrt{1-\overline \alpha_t}\epsilon
}
{
	\sqrt{\overline \alpha_t}
}
$$
代入$q(x_{t-1}|x_t,x_0)$均值化简

![image-20230523201229252](./%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.assets/image-20230523201229252.png)

得

$$
x_{t-1}=
\frac
{
	1
}
{
	\sqrt {\alpha_t}
}
(x_t-\frac
		{
			1-\alpha_t
		}
		{
			\sqrt{1-\overline \alpha_t}
		}
		\epsilon
	)
$$


最小化$\sum_{t=2}^T E_{q(x_t|x_0)}[D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))]$即让两个分布尽可能相似，KL散度越小越好，使两个高斯分布KL散度最小等价于使其均值相似，因此我们可以只预测$\epsilon_t$
$$
x_{t-1}=\frac
{
	1
}
{
	\sqrt {\alpha_t}
}
(x_t-\frac
		{
			1-\alpha_t
		}
		{
			\sqrt{1-\overline \alpha_t}
		}
		\epsilon_{\theta}(x_t,t)
	)+\sigma_tz
$$
![image-20230429013317985](./%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.assets/image-20230429013317985.png)

## 其他场景的应用

- WaveGrad

  语音合成

- Diffusion-LM

  文本生成，添加噪声到隐变量空间

- Diffusioin via Edit-based Reconstruction(EiffusER)

  文本生成，对文本添加mask噪声

## 类似方法

### Mask-Predict

训练一个图像的AutoEncoder

训练时，把一张图片的token某些部分换成masktoken，训练一个BidrectionalTransformer网络，再把mask预测出来

在生成时，使用全部是mask的token为初始，使用BidrectionalTransformer填充mask，并保留置信度高的填充结果，置信度低的再置为mask投入BidrectionalTransformer

## Stable Diffusion

在隐空间下进行ddpm，减小ddpm模型所需要的运算量，并且更加稳定

![image-20230522155757606](./%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.assets/image-20230522155757606.png)







### TextTransformer

使用基于Transofrmer的文本编码器，以实现条件控制，文本生成图像，使用CLIP的结构，可以使用预训练参数

```python
import tensorflow as tf
from tensorflow import keras
import numpy as np

# 默认使用CLIP配置

class TextEmbeddingBlock(keras.layers.Layer):
    """
        文本编码块
        输入为文本的token id和位置id (batch_size,max_text_size，2),
        输出为文本的embedding编码结果 (batch_size,max_text_size,embedding_size)
    """
    def __init__(self, voc_size=49408,max_text_size=77, embedding_dim=768):
        super().__init__()
        self.token_embedding = keras.layers.Embedding(voc_size, embedding_dim,name="token_embedding")
        self.position_embedding = keras.layers.Embedding(max_text_size, embedding_dim,name="position_embedding")

    def call(self, inputs):
        input_ids, position_ids = inputs
        return self.token_embedding(input_ids) + self.position_embedding(position_ids)
  
# class TextMulitiHeadAttention(tf.keras.layers.Layer):
#     def __init__(self,num_heads=12,embedding_dim=768,dropout=0.1):
#         super().__init__()
#         self.embed_dim = 768
#         self.num_heads = 12
#         self.head_dim = self.embed_dim // self.num_heads
#         self.scale = self.head_dim**-0.5
#         self.attn_layer = tf.keras.layers.MultiHeadAttention(num_heads=self.num_heads,
#                                                              key_dim=self.head_dim,
#                                                              value_dim=self.head_dim,
#                                                              dropout=0.1)

#     def call(self, inputs):
#         hidden_states, causal_attention_mask = inputs
#         bsz, tgt_len, embed_dim = hidden_states.shape
#         query_states = hidden_states * self.scale

#         value_states = hidden_states
#         attn_output = self.attn_layer(query_states=query_states,
#                                       value=value_states,
#                                       attention_mask=causal_attention_mask)
#         attn_output = tf.reshape(attn_output, (-1, tgt_len, embed_dim))

#         return attn_output
class CLIPAttention(keras.layers.Layer):
    def __init__(self):
        super().__init__()
        self.embed_dim = 768
        self.num_heads = 12
        self.head_dim = self.embed_dim // self.num_heads
        self.scale = self.head_dim**-0.5
        self.q_proj = keras.layers.Dense(self.embed_dim)
        self.k_proj = keras.layers.Dense(self.embed_dim)
        self.v_proj = keras.layers.Dense(self.embed_dim)
        self.out_proj = keras.layers.Dense(self.embed_dim)

    def _shape(self, tensor, seq_len: int, bsz: int):
        a = tf.reshape(tensor, (bsz, seq_len, self.num_heads, self.head_dim))
        return keras.layers.Permute((2, 1, 3))(a)  # bs , n_head , seq_len , head_dim

    def call(self, inputs):
        hidden_states, causal_attention_mask = inputs
        bsz, tgt_len, embed_dim = hidden_states.shape
        query_states = self.q_proj(hidden_states) * self.scale
        key_states = self._shape(self.k_proj(hidden_states), tgt_len, -1)
        value_states = self._shape(self.v_proj(hidden_states), tgt_len, -1)

        proj_shape = (-1, tgt_len, self.head_dim)
        query_states = self._shape(query_states, tgt_len, -1)
        query_states = tf.reshape(query_states, proj_shape)
        key_states = tf.reshape(key_states, proj_shape)

        src_len = tgt_len
        value_states = tf.reshape(value_states, proj_shape)
        attn_weights = query_states @ keras.layers.Permute((2, 1))(key_states)

        attn_weights = tf.reshape(attn_weights, (-1, self.num_heads, tgt_len, src_len))
        attn_weights = attn_weights + causal_attention_mask
        attn_weights = tf.reshape(attn_weights, (-1, tgt_len, src_len))

        attn_weights = tf.nn.softmax(attn_weights)
        attn_output = attn_weights @ value_states

        attn_output = tf.reshape(
            attn_output, (-1, self.num_heads, tgt_len, self.head_dim)
        )
        attn_output = keras.layers.Permute((2, 1, 3))(attn_output)
        attn_output = tf.reshape(attn_output, (-1, tgt_len, embed_dim))

        return self.out_proj(attn_output)   
    
class TextEncoderBlock(keras.layers.Layer):
    """
        使用attention机制生成文本编码结果
    Args:
        keras (_type_): _description_
    """
    def __init__(self,embedding_dim=768):
        super().__init__()
        self.embedding_dim=embedding_dim
        
        self.layer_norm1= keras.layers.LayerNormalization(epsilon=1e-5)
        self.multi_head_attention=CLIPAttention()
        
        self.layer_norm2=keras.layers.LayerNormalization(epsilon=1e-5)
        
        self.dense1=keras.layers.Dense(4 * self.embedding_dim)
        self.dense2=keras.layers.Dense(self.embedding_dim)
        
    def call(self,inputs):
        hidden_states, attention_mask = inputs 
        residual=hidden_states
    
        hidden_states = self.layer_norm1(hidden_states)
        hidden_states = self.multi_head_attention([hidden_states, attention_mask])
        hidden_states = residual+hidden_states
    
        residual=hidden_states
        hidden_states= self.layer_norm2(hidden_states)
    
        hidden_states = self.dense1(hidden_states)
        hidden_states = hidden_states * tf.sigmoid(hidden_states * 1.702) # qicke_gelu
        hidden_states = self.dense2(hidden_states)
        
        return residual+hidden_states

class TextEncoder(keras.layers.Layer):
    def __init__(self,encoder_block_num=12,embedding_dim=768):
        super().__init__()
        self.layers=[TextEncoderBlock(embedding_dim) for i in range(encoder_block_num)] 
    
    def call(self,inputs):
        [hidden_states, causal_attention_mask] = inputs
        for layer in self.layers:
            hidden_states=layer([hidden_states, causal_attention_mask])
        return hidden_states
                     
class TextTransformer(keras.models.Model):
    """使用EmbeddingBock得到融合位置信息的词向量，使用TextEncoder得到文本编码

    Args:
        keras (_type_): _description_
    """
    def __init__(self, encoder_block_num=12,voc_size=49408,max_text_size=77, embedding_dim=768):
        super().__init__()
        self.embeddings = TextEmbeddingBlock(voc_size=voc_size,max_text_size=max_text_size,embedding_dim=embedding_dim)
        self.encoder = TextEncoder(encoder_block_num)
        self.final_layer_norm = keras.layers.LayerNormalization(epsilon=1e-5)
        self.causal_attention_mask = tf.constant(
            np.triu(np.ones((1, 1, 77, 77), dtype="float32") * -np.inf, k=1)
        )
        # self.causal_attention_mask = tf.linalg.band_part(tf.ones((1, 1, 77, 77))*-tf.inf, -1, 0)

    def call(self, inputs):
        input_ids, position_ids = inputs
        x = self.embeddings([input_ids, position_ids])
        x = self.encoder([x, self.causal_attention_mask])
        return self.final_layer_norm(x)
```




### autoencoder

将图像映射到隐空间下，使生成更加稳定



### diffusion_model




## 参考

### 高斯分布KL散度

两个单一变量的高斯分ff布p和q而言，
$$
KL(p,q)=log\frac{\sigma_2}{\sigma_1}+\frac{\sigma^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}-\frac{1}{2}
$$

### ELBO

ELBO（Evidence Lower Bound）是一种在变分推断中常用的优化目标函数，用于近似计算模型的边缘对数似然。

假设我们有一个概率模型 $p(\mathbf{X}, \mathbf{Z})$，其中 $\mathbf{X}$ 是观测数据，$\mathbf{Z}$ 是潜在变量。我们希望计算 $p(\mathbf{X})$ 的对数似然 $\log p(\mathbf{X})$。但是，由于 $\mathbf{Z}$ 是未知的，我们无法直接计算边缘概率 $p(\mathbf{X})$。变分推断是一种用来逼近边缘概率分布的方法，它利用一个近似分布 $q(\mathbf{Z})$ 来近似真实的后验分布 $p(\mathbf{Z}|\mathbf{X})$。

ELBO 是一种用来评价近似分布 $q(\mathbf{Z})$ 和真实后验分布 $p(\mathbf{Z}|\mathbf{X})$ 之间的差异的指标。ELBO 定义为：

$$\text{ELBO}(q) = \mathbb{E}_{q(\mathbf{Z})}[\log p(\mathbf{X}, \mathbf{Z}) - \log q(\mathbf{Z})]$$

其中，$\mathbb{E}_{q(\mathbf{Z})}$ 表示对 $q(\mathbf{Z})$ 取期望。ELBO 可以被理解为真实后验分布的对数似然 $\log p(\mathbf{X})$ 的下界。具体来说，ELBO 满足下面的不等式：

$$\log p(\mathbf{X}) \geq \text{ELBO}(q)$$

也就是说，ELBO 给出了一个下界，它可以用来近似计算 $\log p(\mathbf{X})$。



log p(x)下界求得如下

### 参数重整化

希望从高斯分布$N(\mu,\sigma)$中采样，可以先从标注只能分布$N(0,1)$采样出Z，在得到$\sigma z+\mu$，这样做将随机性转移到z这个常量上，而$\sigma,\mu$当做仿射变换网络的一部分
