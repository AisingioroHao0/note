# 梯度下降法(gradient descent)

## 基本原理

![image-20230119115326796](./%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.assets/image-20230119115326796.png)

对于含参数w的函数J(w)，求J（w）的最小值：

$\frac{dJ(w)}{dw}$即J在w处的斜率。

当斜率为正w增大J增大，w需要减小。

当斜率为小w增大J减小，w需要增大。

因此前面需要取负。



![image-20230119120744337](./%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.assets/image-20230119120744337.png)

学习率过大容易在不同单调区间反复横跳导致无法逼近到极小值。



每步使用所有训练数据的梯度下降称为批量梯度下降("Batch" gradient decent)。



## 使用tensorflow实现梯度下降

![image-20230219152918171](./%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.assets/image-20230219152918171.png)

```python
import tensorflow as tf
w = tf.Variable(3.0)
x = 1.0
y = 1.0
alpha = 0.01
iterations = 30
for i in range(iterations):
    with tf.GradientTape() as tape:
        fwb = w*x
        costJ = (fwb - y)**2

    [dJdw] = tape.gradient(costJ, [w])
    w.assigin_add(-alpha*dJdw)

```

## 传统梯度下降法训练缺点

- 梯度越来越稀疏
- 收敛到局部最小值
- 只能监督学习

