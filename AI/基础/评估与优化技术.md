# 评估与优化技术

## 欠拟合

模型欠拟合(underfit）或者称这个算法有较高得偏差（high bias）。

优化方法

添加特征

更大的神经网络

减小lambda

## 过拟合

模型过拟合(overfit)或者称这个算法有较高得方差。



![image-20230209210502732](./%E8%AF%84%E4%BC%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF.assets/image-20230209210502732.png)

优化方法：

- 使用更多的训练数据
- 减少训练用得特征
- 减少参数得尺寸
- 添加正则化

![image-20230130003015221](./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.assets/image-20230130003015221.png)

![image-20230130003536513](./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.assets/image-20230130003536513.png)

### L2正则化

减少高阶项得影响

![image-20230130005101159](./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.assets/image-20230130005101159.png)

核心思想是对经验函数(J，cost function)新增关于w的项，因为我们希望最小化J，那么大的系数会使对应得w变小。

![image-20230130010801416](./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.assets/image-20230130010801416.png)

如：

![image-20230130011137135](./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.assets/image-20230130011137135.png)

会使所有得w都会变小。

如果$\lambda$为很小则无效。如果$\lambda$很大则会使所有得w趋近于0，模型则趋近于f(x)=b。

![image-20230209210339538](./%E8%AF%84%E4%BC%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF.assets/image-20230209210339538.png)



![image-20230209201134421](./%E8%AF%84%E4%BC%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF.assets/image-20230209201134421.png)



### 

在神经网络之中，如tanh激活函数，w更小则每个神经元的值更趋近于线性函数，不容易拟合更复杂的函数，可以避免过拟合。

### Dropout Regularization

随机失活正则化，随机使神经元的激活值为0.

```python
di=np.random.arnd(ai.shape[0],ai.shape[1])<keep_prob
ai=ai@di
ai/=keep_prob # 避免下一层的期望值尺度不一样 inverted dropout
```

对于每个神经元，不依赖于上一层任何单个特征，因为每个神经元的值都有可能被丢弃掉，权重得到分散。

### 制造新的数据

数据加工，如图片的裁剪缩放镜像翻转

### 提前终止

绘制开发集和训练集的曲线，在开发集代价上升时停止训练

## 梯度下降优化

特征缩放，如归一化，可以加快求解速度。

个人关于特征缩放可加快求解速度得解释是：

因为损失函数是和训练数据相关的，训练数据的尺度决定了损失函数随训练参数的变化尺度。

如果每个特征的尺度不一样，但每个模型参数变化量级又是固定的，导致每个参数对损失函数的影响程度会不同，如果某个参数对损失函数的影响额外的大，可能会导致损失函数在关于该参数得不同单调区间反复横跳，导致训练速度慢。

![image-20230128000005666](./%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.assets/image-20230128000005666.png)

归一化

### 特征缩放

#### 均值归一化

$$
x_i=\frac{x_i-\mu_i}{max(x_i)-min(x_i)}
$$

表示和均值得差异

#### Z-score 归一化

$$
x_i=\frac{x_i-\mu_i}{\sigma_i}
$$

![image-20230128232650773](./%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.assets/image-20230128232650773.png) 



### 随机梯度下降法

每次下降使用某个数据进行梯度下降，优化某个数据的损失函数，而非所有数据。

优点：加快训练速度

缺点：

- 准确度下降
- 可能收敛到局部最优，单个样本不足以表现所有样本的下降趋势
- 不易于并行实现

### mini-batch梯度下降法

训练集分割为多个小的训练集，每次使用一个训练集进行训练

### Gradient Descent with Momentum

动量梯度下降法，利用指数加权平均法使梯度更加平滑，减缓振荡
$$
V_{dx}=\beta V_{dx}+(1-\beta)dx \\
x-=\alpha V_{dx}
$$



### RMSprop

在动量梯度算法上改进，使大的梯度变小，小的梯度变大
$$
S_{dx}=\beta S_{dx}+(1-\beta)(dx)^2 \\
x-=\alpha \frac{dx}{\sqrt {S_{dx}}}
$$

### adam algorithm

adam：adaptive moment estimation

结合RMSprop与动量梯度
$$
V_{dx}=\beta_1 V_{dx}+(1-\beta_1)dx \\
S_{dx}=\beta_2 S_{dx}+(1-\beta_2)(dx)^2 \\
W=W-\alpha\frac{V_{dw}}{\sqrt{S_{dw}}+\epsilon}
$$


 并使学习率逐渐下降

- epoch_num为梯度下降的次数
- decay_rate

有多种衰减方式：
$$
\alpha=\frac{1}{1+decay\_rate*epoch\_num}\alpha_0 \hfill\\
\alpha=decay\_rate^{epoch\_num}\alpha_0\hfill\\
\alpha=\frac{k}{\sqrt{epoch\_num}}\alpha_0\hfill\\
$$

### batch normalizing

将每层的z值归一化并赋予新的参数使之得到学习能力,学习$\gamma$,$\beta$。

减少了隐藏神经单元的不稳定性，减少了输入值变化产生的不稳定性，增加了神经网络后层的稳定性。

若采用batch normalizing，原先神经网络参数b将无用。

在使用mini-batch时，对每一个mini-batch计算方差于均值进行归一化，方差于均值将产生一定噪声，因此具有轻微正则化的效果。



当一个一个样例测试时，可以使用指数加权平均法估计平均值

## 梯度消失/爆炸

当神经网络的层数非常多时，初始化W>I则参数会指数级增长，初始化W<I时参数会指数级减少。



因为
$$
a[i][j]=\sum_{j=1}^{n[i-1]}w[i][j]a[i-1][j]
$$
所以期望特征维度越高，值越小。

### 改变初始化方式

当使用relu作为激活函数时，更改初始化w的方式为

```python
W[l]=np.random.randn(shape)*np.sqrt(2/n[l-1])
```

当采用tanh作为激活函数时，改为*np.sqrt(1\*/n[l-1])或

\*np.sqrt(2/(n[l-1]*n[l]))  (Xavier initialization)

## 超参数估计

### 学习率

通常认为学习率在[1e-4,1]之间，

```python
r=-4*np.random.rand()
learning_rate=10**r
```

### 指数平均优化梯度参数

指数平均优化梯度参数，通常认为在[0.9,0.999]之间,可以取随机1-$\beta_1$，在[0.1,0.001]之间

```python
r=-1+-2*np.random.rand() # -1+[0,-2]
beta1=1-10**r #[0.9,0.]
```



## 神经网络

![image-20230209234925402](./%E8%AF%84%E4%BC%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF.assets/image-20230209234925402.png)

 

## 梯度检查

通过模拟计算微分的值和反向传播计算的值作差比较：

<=1e-7则基本是正确的

\>=1e-3则基本是错误的

## 优化方向

以人类水平作为基准

训练误差大于人类水平的部分称为可避免的偏差。

测试误差大于训练误差的部分称为方差variance。

## 错误分析

![image-20230305163441539](./%E8%AF%84%E4%BC%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF.assets/image-20230305163441539.png)

![image-20230306133804049](./%E8%AF%84%E4%BC%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF.assets/image-20230306133804049.png)

