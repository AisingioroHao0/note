# 支持向量机

求切分的超平面，并且使每个划分的确信度达到最高，即距离超平面最近的点距离最远

- 函数间隔

  ![image-20230314104118229](./%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.assets/image-20230314104118229.png)

- 几何间隔

  同比例增大wb，超平面没有改变，但函数间隔改变，几何间隔不变

  ![image-20230314104144006](./%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.assets/image-20230314104144006.png)

## 间隔最大化

![image-20230314104454889](./%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.assets/image-20230314104454889.png)

使几何间隔最大，可改写成

![image-20230314104545717](./%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.assets/image-20230314104545717.png)

函数间隔$\hat\gamma$不影响最优化问题的解。

假设将w，b按比例改变为$\lambda w,\lambda b$，这是函数间隔成了$\lambda \hat \gamma$，但是目标$\frac{\hat \gamma}{||w||}$依旧没有变。因此$\hat \lambda$取值与优化无关

可令$\hat \gamma=1$,同时最大化$\frac{1}{||w||}$和最小化$\frac{1}{2}||w||^2$使等价的

因此得到目标：
$$
min_{w,b} \frac{1}{2}||w||^2 \hfill \\
s.t.\ y_i(w \cdot x +b)-1>=0
$$


![image-20230314110354059](./%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.assets/image-20230314110354059.png)

这是一个凸二次规划问题

## 学习算法

对于求解线性可分支持向量机的最优化问题，可以使用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解，引入$\alpha$作为拉格朗日乘子

$$
L(w,b,\alpha)=\frac{1}{2}||w||^2+\sum_{i=1}^N(1-y_i(w\cdot x_i+b))\alpha_i
$$

得到线性可分支持向量机学习算法

![image-20230316165327909](./%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.assets/image-20230316165327909.png)


## 证明

![image-20230314112309500](./%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.assets/image-20230314112309500.png)

![image-20230314112327970](./%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.assets/image-20230314112327970.png)

## 支持向量

在线性可分的情况下，训练数据集的样本点中与分离超平面距离最近的样本点实例称为支持向量，使满足等号成立的点，即
$$
y_i(w\cdot x_i+b)-1=0
$$
对于$y_i=+1$的实例点，支持向量在超平面
$$
H_1:w\cdot x  +b=1
$$
对于$y_i=-1$的实例点，支持向量在超平面
$$
H_1:w\cdot x  +b=-1
$$
![image-20230314112755938](./%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.assets/image-20230314112755938.png)

$H_1$与$H_2$平行，之间的距离称为间隔，间隔依赖于超平面的法向量w,等于$\frac{2}{||w||}$,$H_1$和$H_2$称为间隔边界

决定分离超平面只有支持向量起作用，因此支持向量机由很少的“重要的”训练样本确定。

支持向量对应$\alpha^*>0$。

根据KKT有
$$
\alpha^*(y_i(w^*\cdot x_i+b^*)-1)=0 \hfill \\
y_i(w^*\cdot x_i+b^*)-1=0 \hfill \\
w^*\cdot x_i+b^*=\pm1
$$

## 软间隔最大化

对于线性不可分数据集，宽松化约束条件，引入松弛变量$\xi$，允许一些点可以越界

![image-20230316172725611](./%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.assets/image-20230316172725611.png)

同时又希望松弛尽可能小，划分尽可能严密，修改目标函数为
$$
\frac{1}{2}||w^2||+C\sum_{i=1}^N \xi_i
$$
C>0称为惩罚参数，一般有引用问题决定。C值大时对误分类的惩罚增大，C值小时对误分类的惩罚减小。

最小化该目标函数有两层含义：

-  $\frac{1}{2}||w^2||$尽量小及间隔尽量阿达
- $\sum_{i=1}^N \xi_i$尽量小及误分类点的个数尽量少

C是调和二者的系数

由拉格朗日对偶问题可得算法：

![image-20230316175709153](./%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.assets/image-20230316175709153.png)



若$\alpha^*=C,0<\xi_i<1$，则分类正确，$x_i$在间隔边界与分离超平面之间

若$\alpha_i^*<C$则$\xi_i=0$,支持向量$x_i$正好落在间隔边界上

## 非线性支持向量机

使用核函数

![image-20230319183540321](./%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.assets/image-20230319183540321.png)

## 核函数

### 多项式核函数

对应
$$
K(x,z)=(x\cdot z+1)^p
$$
决策函数变为

![image-20230319185457317](./%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.assets/image-20230319185457317.png)

### 高斯核函数

![image-20230319185335276](./%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.assets/image-20230319185335276.png)

![image-20230319185620176](./%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA.assets/image-20230319185620176.png)

### 字符串核函数
